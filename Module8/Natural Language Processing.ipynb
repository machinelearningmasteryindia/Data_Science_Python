{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Natural Language Processing.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"TbdV3WNHoioM","colab_type":"text"},"source":["# **Natural Language Processing**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"iz37w6sNownh","colab_type":"text"},"source":["## **What Is Natural Language Processing?**\n","\n","Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data....([Wikipedia)](https://en.wikipedia.org/wiki/Natural_language_processing)\n","\n","By utilizing NLP, developers can organize and structure knowledge to perform tasks such as automatic summarization, translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation."]},{"cell_type":"markdown","metadata":{"id":"tLfwMCfip-KH","colab_type":"text"},"source":["## **Use Cases of NLP**\n","\n","In simple terms, NLP represents the automatic handling of natural human language like speech or text, and although the concept itself is fascinating, the real value behind this technology comes from the use cases.\n","\n","NLP can help you with lots of tasks and the fields of application just seem to increase on a daily basis. Let’s mention some examples:\n","\n"," 1. NLP enables the recognition and prediction of diseases based on electronic health records and patient’s own speech. This capability is being explored in health conditions that go from cardiovascular diseases to depression and even schizophrenia. For example, Amazon Comprehend Medical is a service that uses NLP to extract disease conditions, medications and treatment outcomes from patient notes, clinical trial reports and other electronic health records.\n","\n"," 2. Organizations can determine what customers are saying about a service or product by identifying and extracting information in sources like social media. This sentiment analysis can provide a lot of information about customers choices and their decision drivers.\n","\n"," 3. Companies like Yahoo and Google filter and classify your emails with NLP by analyzing text in emails that flow through their servers and stopping spam before they even enter your inbox.\n","\n"," 4. To help identifying fake news, the NLP Group at MIT developed a new system to determine if a source is accurate or politically biased, detecting if a news source can be trusted or not.\n","\n"," 5. Amazon’s Alexa and Apple’s Siri are examples of intelligent voice driven interfaces that use NLP to respond to vocal prompts and do everything like find a particular shop, tell us the weather forecast, suggest the best route to the office or turn on the lights at home.\n","\n"," 6. Having an insight into what is happening and what people are talking about can be very valuable to financial traders. NLP is being used to track news, reports, comments about possible mergers between companies, everything can be then incorporated into a trading algorithm to generate massive profits. Remember: buy the rumor, sell the news.\n","\n"," 7. NLP is also being used in both the search and selection phases of talent recruitment, identifying the skills of potential hires and also spotting prospects before they become active on the job market.\n","\n"," 8. Chatbots: Virtual personal assistants also are known as chatbots are rapidly making their presence in the digital world. Businesses are using chatbots across support, marketing, healthcare verticals.\n","\n"," 9. Speech Recognition: This is where devices like Alexa, Siri, Google home and any other virtual assistants come to picture. NLP has developed its roots in healthcare with speech recognition, allowing clinicians to transcribe notes for efficient EHR data entry for nearly two decades.\n","\n"," 10. Credit worthiness assessment: Nowadays many banks and lending companies leveraging NLP and assess the credit worthiness of clients with little or no credit history. For example, students who got a job first time and start earning money have no or little credit history. But they are potential customers to banks for giving loans. Even if these clients have never used credit before, most of them still use smartphones, browse the internet and engage in other activities that leave a lot of digital footprints. NLP algorithms analyze geolocation data, social media activity, browsing behaviour to derive insights into their habits, peer networks, and strength of their relationships. By analyzing thousands of client-related variables, the software generates a credit score highly predictive of customer’s further activity.\n","\n"," 11. Neural Machine Translation: What has previously seemed like an awkward attempt to imitate the professional translation has now substantially improved, but neural machine translation (NMT)has taken the improvements even further. Google, Amazon, and Microsoft are competing to deliver the best machine translation today."]},{"cell_type":"markdown","metadata":{"id":"cRFED9Igqgu3","colab_type":"text"},"source":["## **Basic concepts in NLP**"]},{"cell_type":"markdown","metadata":{"id":"TC518GVetKAH","colab_type":"text"},"source":["### **What is Language?**\n","\n","A language, basically is a fixed vocabulary of words which is shared by a community of humans to express and communicate their thoughts.\n","This vocabulary is taught to humans as a part of their growing up process, and mostly remains fixed with few additions each year.\n","Elaborate resources such as dictionaries are maintained so that if a person comes across a new word he or she can reference the dictionary for its meaning. Once the person gets exposed to the word it gets added in his or her vocabulary and can be used for further communications."]},{"cell_type":"markdown","metadata":{"id":"37_P6MqbtLVe","colab_type":"text"},"source":["### **How do computers understand language?**\n","\n","A computer is a machine working under mathematical rules. It lacks the complex interpretations and understandings which humans can do with ease, but can perform a complex calculation in seconds.\n","\n","> For a computer to work with any concept it is necessary that there should be a way to express the said concept in the form of a mathematical model.\n","\n","This constraint highly limits the scope and the areas of natural language a computer can work with. So far what machines have been highly successful in performing are classification and translation tasks.\n"]},{"cell_type":"markdown","metadata":{"id":"fJJlADP_tYis","colab_type":"text"},"source":["### **Basic Transformations**\n","\n","As mentioned earlier, for a machine to make sense of natural language( language used by humans) it needs to be converted into some sort of a mathematical framework which can be modeled. Below mentioned, are some of the most commonly used techniques which help us achieve that."]},{"cell_type":"markdown","metadata":{"id":"XwZgj2zAAwD2","colab_type":"text"},"source":["#### **Data cleaning**\n","\n","There are some basic cleaning steps which can also helps improve effectiveness of NLP system, however it should be used carefully and case by case basis. These are:\n","\n"," + converting into lowercase\n"," + Removing punctuations\n"," + Removing special characters and html tags like @, # etc.a"]},{"cell_type":"markdown","metadata":{"id":"gK7OMD7WtkWG","colab_type":"text"},"source":["#### **Tokenization**\n","\n","Tokenization is the process of segmenting running text into sentences and words. In essence, it’s the task of cutting a text into pieces called tokens, and at the same time throwing away certain characters, such as punctuation. \n","\n","To bring a short example let's look at the first sentence of the song “Across the Universe” from The Beatles:\n","\n","> Words are flowing out like endless rain into a paper cup,\n","\n",">  They slither while they pass, they slip away across the universe\n","\n","Now,the result of tokenization would be:\n","\n","`Words` `are` `flowing` `out` `like` `endless` `rain` `into` `a` `paper` `cup`\n","\n","`They` `slither` `while` `they` `pass`, `they` `slip` `away` `across` `the` `universe`\n","\n","Although it may seem quite basic in this case and also in languages like English that separate words by a blank space (called segmented languages) not all languages behave the same, and if you think about it, blank spaces alone are not sufficient enough even for English to perform proper tokenizations. Splitting on blank spaces may break up what should be considered as one token, as in the case of certain names (e.g. San Francisco or New York) or borrowed foreign phrases (e.g. laissez faire).\n","\n","**Tokenization can remove punctuation too**, easing the path to a proper word segmentation but also triggering possible complications. In the case of periods that follow abbreviation (e.g. dr.), the period following that abbreviation should be considered as part of the same token and not be removed.\n","\n","The tokenization process can be particularly problematic when dealing with biomedical text domains which contain lots of hyphens, parentheses, and other punctuation marks."]},{"cell_type":"markdown","metadata":{"id":"z2aoRdn-vHL7","colab_type":"text"},"source":["#### **Stop Words**\n","\n","Stop Words Removal includes getting rid of common language articles, pronouns and prepositions such as “and”, “the” or “to” in English. In this process some very common words that appear to provide little or no value to the NLP objective are filtered and excluded from the text to be processed, hence removing widespread and frequent terms that are not informative about the corresponding text.\n","\n","Stop words can be safely ignored by carrying out a lookup in a pre-defined list of keywords, freeing up database space and improving processing time.\n","\n","> There is no universal list of stop words. \n","\n","These can be pre-selected or built from scratch. A potential approach is to begin by adopting pre-defined stop words and add words to the list later on. Nevertheless it seems that the general trend over the past time has been to go from the use of large standard stop word lists to the use of no lists at all.\n","\n","The thing is stop words removal can wipe out relevant information and modify the context in a given sentence. For example, if we are performing a sentiment analysis we might throw our algorithm off track if we remove a stop word like “not”. Under these conditions, you might select a minimal stop word list and add additional terms depending on your specific objective."]},{"cell_type":"markdown","metadata":{"id":"xgWqvatYv5Zi","colab_type":"text"},"source":["#### **Stemming**\n","\n","Stemming refers to the process of slicing the end or the beginning of words with the intention of removing affixes (lexical additions to the root of the word).\n","\n","> Affixes that are attached at the beginning of the word are called prefixes (e.g. “astro” in the word “astrobiology”) and the ones attached at the end of the word are called suffixes (e.g. “ful” in the word “helpful”).\n","\n","Stemming is a kind of normalization for words. Normalization is a technique where a set of words in a sentence are converted into a sequence to shorten its lookup. The words which have the same meaning but have some variation according to the context or sentence are normalized.\n","Generally, there is one root word, but there are many variations of the same words. For example, the root word is \"eat\" and it's variations are \"eats, eating, eaten and like so\". In the same way, with the help of Stemming, we can find the root word of any variations. In short, Stemming is a rudimentary rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word.\n","\n","However, Stemming has serious limitations as sometime it change context of words altogether. For ex, **News** can be stemmed into **New** so why do we use it? First of all, it can be used to correct spelling errors from the tokens. Stemmers are simple to use and run very fast (they perform simple operations on a string), and if speed and performance are important in the NLP model, then stemming is certainly the way to go. Remember, we use it with the objective of improving our performance, not as a grammar exercise."]},{"cell_type":"markdown","metadata":{"id":"63aOfjxPxXK0","colab_type":"text"},"source":["#### **Lemmatization**\n","\n","Lemmatization has the objective of reducing a word to its base form and grouping together different forms of the same word. For example, verbs in past tense are changed into present (e.g. “went” is changed to “go”) and synonyms are unified (e.g. “best” is changed to “good”), hence standardizing words with similar meaning to their root. Although it seems closely related to the stemming process, lemmatization uses a different approach to reach the root forms of words.\n","\n","> Lemmatization resolves words to their dictionary form (known as lemma) for which it requires detailed dictionaries in which the algorithm can look into and link words to their corresponding lemmas.\n","\n","For example, the words “running”, “runs” and “ran” are all forms of the word “run”, so “run” is the lemma of all the previous words.\n","\n","Let's look at comparison of Lemmatization and Stemming for the word **Caring**.\n","\n","$$Lemmatization : Caring -> Care$$ \n","\n","$$Stemming : Caring -> Car$$ \n","\n","Lemmatization also takes into consideration the context of the word in order to solve other problems like disambiguation, which means it can discriminate between identical words that have different meanings depending on the specific context. Think about words like “bat” (which can correspond to the animal or to the metal/wooden club used in baseball) or “bank” (corresponding to the financial institution or to the land alongside a body of water). By providing a part-of-speech parameter to a word ( whether it is a noun, a verb, and so on) it’s possible to define a role for that word in the sentence and remove disambiguation.\n","\n","As you might already pictured, lemmatization is a much more resource-intensive task than performing a stemming process. At the same time, since it requires more knowledge about the language structure than a stemming approach, it demands more computational power than setting up or adapting a stemming algorithm.\n","\n","> Since stemming occurs based on a set of rules, the root word returned by stemming might not always be a word of the english language. Lemmatization on the other hand reduces the inflected words properly ensuring that the root word belongs to english language."]},{"cell_type":"markdown","metadata":{"id":"BjSs49iIz9rW","colab_type":"text"},"source":["#### **N-Grams**\n","N-grams refer to the process of combining the nearby words together for representation purposes where N represents the number of words to be combined together.\n","\n","For ex, consider first sentence from above example \"Words are flowing out like endless rain into a paper cup\".\n","\n","A 1-gram or unigram model will tokenize the sentence into one word combinations and thus the output will be \"Words, are, flowing, out, like, endless, rain, into, a, paper, cup.\"\n","\n","A bigram model on the other hand will tokenize it into combination of 2 words each and the output will be \"Words are, are flowing, flowing out, out like, like endless, endless rain, rain into, into a, a paper, paper cup\".\n","\n","Similarly, a trigram model will break it into \"Words are flowing, are flowing out, flowing out like, out like endless, like endless rain, endless rain into, ran into a, into a paper. and a n-gram model will thus tokenize a sentence into combination of n words together.\n","\n","> Breaking down a natural language into n-grams is essential for maintaining counts of words occurring in sentences which forms the backbone of traditional mathematical processes used in Natural Language Processing."]},{"cell_type":"markdown","metadata":{"id":"_kMMnwVSzHnQ","colab_type":"text"},"source":["### **Mathematical transformation for solving NLP**\n","\n","As computer can understand only Mathematical forms, it is necessary to convert text/natural languages into mathematical forms. Some of most popular method used in NLP are:\n","\n"," 1. One-Hot Encodings\n"," 2. Bag of words\n"," 3. TF-TDF\n"," 4. Word embeddings"]},{"cell_type":"markdown","metadata":{"id":"ZmzGcuGR5CB3","colab_type":"text"},"source":["#### **One-Hot Encodings**\n","\n","One hot encodings are another way of representing words in numeric form. The length of the word vector is equal to the length of the vocabulary, and each observation is represented by a matrix with rows equal to the length of vocabulary and columns equal to the length of observation, with a value of 1 where the word of vocabulary is present in the observation and a value of zero where it is not.\n","\n"]},{"cell_type":"code","metadata":{"id":"ME5N8DcDNvZS","colab_type":"code","colab":{}},"source":["Str1=\"Words are flowing out like endless rain into a paper cup\"\n","\n","Str2=\"They slither while they pass, they slip away across the universe\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0kYR2phKN6WT","colab_type":"code","outputId":"545654aa-4410-47a3-f639-404ec1786084","executionInfo":{"status":"ok","timestamp":1587900918866,"user_tz":-330,"elapsed":1436,"user":{"displayName":"Ravi Mandal","photoUrl":"","userId":"13495112489923952431"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Making lowercase\n","Str1=Str1.lower()\n","# Removing Punctuations, Numbers, and Special Characters\n","Str1=Str1.replace(\"[^a-zA-Z]\", \" \")\n","#Tokenization\n","Str1=Str1.split()\n","print(Str1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['words', 'are', 'flowing', 'out', 'like', 'endless', 'rain', 'into', 'a', 'paper', 'cup']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9eLR1hAQP8GA","colab_type":"code","outputId":"f9267c08-c63d-4e97-e74c-ad0cd0b45c70","executionInfo":{"status":"ok","timestamp":1587900918872,"user_tz":-330,"elapsed":1395,"user":{"displayName":"Ravi Mandal","photoUrl":"","userId":"13495112489923952431"}},"colab":{"base_uri":"https://localhost:8080/","height":230}},"source":["import numpy as np\n","X=np.array(Str1)\n","X=X.reshape(-1, 1)\n","print(X.shape)\n","print(X)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(11, 1)\n","[['words']\n"," ['are']\n"," ['flowing']\n"," ['out']\n"," ['like']\n"," ['endless']\n"," ['rain']\n"," ['into']\n"," ['a']\n"," ['paper']\n"," ['cup']]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"He9oB5enPXtc","colab_type":"code","outputId":"e1041c10-dc88-4077-ad5b-e0a2186cde1c","executionInfo":{"status":"ok","timestamp":1587900924665,"user_tz":-330,"elapsed":1809,"user":{"displayName":"Ravi Mandal","photoUrl":"","userId":"13495112489923952431"}},"colab":{"base_uri":"https://localhost:8080/","height":212}},"source":["from sklearn.preprocessing import OneHotEncoder\n","enc = OneHotEncoder(handle_unknown='ignore')\n","X=enc.fit_transform(X)\n","print(X)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["  (0, 10)\t1.0\n","  (1, 1)\t1.0\n","  (2, 4)\t1.0\n","  (3, 7)\t1.0\n","  (4, 6)\t1.0\n","  (5, 3)\t1.0\n","  (6, 9)\t1.0\n","  (7, 5)\t1.0\n","  (8, 0)\t1.0\n","  (9, 8)\t1.0\n","  (10, 2)\t1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2ajPnB3SRrU8","colab_type":"code","outputId":"93fbb7de-7f43-46c8-8164-018ecdafb52c","executionInfo":{"status":"ok","timestamp":1587901081637,"user_tz":-330,"elapsed":1140,"user":{"displayName":"Ravi Mandal","photoUrl":"","userId":"13495112489923952431"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["enc.categories_"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array(['a', 'are', 'cup', 'endless', 'flowing', 'into', 'like', 'out',\n","        'paper', 'rain', 'words'], dtype='<U7')]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"5ilCiNFeIUEI","colab_type":"code","outputId":"feb99572-3e60-4c8f-f82e-dbb8ff25e120","executionInfo":{"status":"ok","timestamp":1587915489549,"user_tz":-330,"elapsed":926,"user":{"displayName":"Ravi Mandal","photoUrl":"","userId":"13495112489923952431"}},"colab":{"base_uri":"https://localhost:8080/","height":107}},"source":["import numpy as np\n","import pandas as pd\n","\n","data=pd.DataFrame(Corpus,columns=['Text'])\n","data"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Words are flowing out like endless rain into a...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>They slither while they pass, they slip away a...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                Text\n","0  Words are flowing out like endless rain into a...\n","1  They slither while they pass, they slip away a..."]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"yHNLDxI8I8CH","colab_type":"code","outputId":"11391ea2-db59-4e10-d370-8a684e137152","executionInfo":{"status":"ok","timestamp":1587915591786,"user_tz":-330,"elapsed":1002,"user":{"displayName":"Ravi Mandal","photoUrl":"","userId":"13495112489923952431"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["tokenized_tweet = data['Text'].apply(lambda x: x.split())\n","tokenized_tweet"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    [Words, are, flowing, out, like, endless, rain...\n","1    [They, slither, while, they, pass,, they, slip...\n","Name: Text, dtype: object"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"r38LI2SnTBjZ","colab_type":"text"},"source":["#### Bag of Words"]},{"cell_type":"code","metadata":{"id":"lqZyC9BeS_hf","colab_type":"code","colab":{}},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","Corpus=[\"Words are flowing out like endless rain into a paper cup\",\"They slither while they pass, they slip away across the universe\"]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e2fy6KevOps2","colab_type":"code","outputId":"abd16c3e-117e-44b5-c215-26a1039cd79d","executionInfo":{"status":"ok","timestamp":1587902297747,"user_tz":-330,"elapsed":1393,"user":{"displayName":"Ravi Mandal","photoUrl":"","userId":"13495112489923952431"}},"colab":{"base_uri":"https://localhost:8080/","height":107}},"source":["vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(Corpus)\n","# print(vectorizer.get_feature_names())\n","# print(X.toarray())\n","import pandas as pd\n","data=pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names())\n","data"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>across</th>\n","      <th>are</th>\n","      <th>away</th>\n","      <th>cup</th>\n","      <th>endless</th>\n","      <th>flowing</th>\n","      <th>into</th>\n","      <th>like</th>\n","      <th>out</th>\n","      <th>paper</th>\n","      <th>pass</th>\n","      <th>rain</th>\n","      <th>slip</th>\n","      <th>slither</th>\n","      <th>the</th>\n","      <th>they</th>\n","      <th>universe</th>\n","      <th>while</th>\n","      <th>words</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   across  are  away  cup  endless  ...  the  they  universe  while  words\n","0       0    1     0    1        1  ...    0     0         0      0      1\n","1       1    0     1    0        0  ...    1     3         1      1      0\n","\n","[2 rows x 19 columns]"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"OwSpVXE8V5-p","colab_type":"code","outputId":"9861d9ec-08cb-4dba-da69-95e0cd638a09","executionInfo":{"status":"ok","timestamp":1587902702026,"user_tz":-330,"elapsed":1403,"user":{"displayName":"Ravi Mandal","photoUrl":"","userId":"13495112489923952431"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n","X2 = vectorizer2.fit_transform(Corpus)\n","# print(vectorizer2.get_feature_names())\n","# print(X2.toarray())\n","\n","data2=pd.DataFrame(X2.toarray(),columns=vectorizer2.get_feature_names())\n","data2"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>across the</th>\n","      <th>are flowing</th>\n","      <th>away across</th>\n","      <th>endless rain</th>\n","      <th>flowing out</th>\n","      <th>into paper</th>\n","      <th>like endless</th>\n","      <th>out like</th>\n","      <th>paper cup</th>\n","      <th>pass they</th>\n","      <th>rain into</th>\n","      <th>slip away</th>\n","      <th>slither while</th>\n","      <th>the universe</th>\n","      <th>they pass</th>\n","      <th>they slip</th>\n","      <th>they slither</th>\n","      <th>while they</th>\n","      <th>words are</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   across the  are flowing  away across  ...  they slither  while they  words are\n","0           0            1            0  ...             0           0          1\n","1           1            0            1  ...             1           1          0\n","\n","[2 rows x 19 columns]"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"l8Qcl_40ZWm5","colab_type":"code","outputId":"0afb4985-ac6d-4e06-fcf6-96e6b574327e","executionInfo":{"status":"ok","timestamp":1587903069462,"user_tz":-330,"elapsed":1458,"user":{"displayName":"Ravi Mandal","photoUrl":"","userId":"13495112489923952431"}},"colab":{"base_uri":"https://localhost:8080/","height":141}},"source":["vectorizer3 = CountVectorizer(analyzer='word', ngram_range=(3, 3))\n","X3 = vectorizer3.fit_transform(Corpus)\n","# print(vectorizer3.get_feature_names())\n","# print(X3.toarray())\n","\n","data3=pd.DataFrame(X3.toarray(),columns=vectorizer3.get_feature_names())\n","data3"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>across the universe</th>\n","      <th>are flowing out</th>\n","      <th>away across the</th>\n","      <th>endless rain into</th>\n","      <th>flowing out like</th>\n","      <th>into paper cup</th>\n","      <th>like endless rain</th>\n","      <th>out like endless</th>\n","      <th>pass they slip</th>\n","      <th>rain into paper</th>\n","      <th>slip away across</th>\n","      <th>slither while they</th>\n","      <th>they pass they</th>\n","      <th>they slip away</th>\n","      <th>they slither while</th>\n","      <th>while they pass</th>\n","      <th>words are flowing</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   across the universe  are flowing out  ...  while they pass  words are flowing\n","0                    0                1  ...                0                  1\n","1                    1                0  ...                1                  0\n","\n","[2 rows x 17 columns]"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"hpzbR6ZTZwQE","colab_type":"text"},"source":["#### TF-IDF"]},{"cell_type":"code","metadata":{"id":"ahtvXWQ-ZzIq","colab_type":"code","outputId":"75f7fd97-d981-4d5d-f6e5-afd01a239392","executionInfo":{"status":"ok","timestamp":1587903410911,"user_tz":-330,"elapsed":1470,"user":{"displayName":"Ravi Mandal","photoUrl":"","userId":"13495112489923952431"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","vectorizer = TfidfVectorizer()\n","X = vectorizer.fit_transform(Corpus)\n","# print(vectorizer.get_feature_names())\n","\n","dataTF=pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names())\n","dataTF"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>across</th>\n","      <th>are</th>\n","      <th>away</th>\n","      <th>cup</th>\n","      <th>endless</th>\n","      <th>flowing</th>\n","      <th>into</th>\n","      <th>like</th>\n","      <th>out</th>\n","      <th>paper</th>\n","      <th>pass</th>\n","      <th>rain</th>\n","      <th>slip</th>\n","      <th>slither</th>\n","      <th>the</th>\n","      <th>they</th>\n","      <th>universe</th>\n","      <th>while</th>\n","      <th>words</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000000</td>\n","      <td>0.316228</td>\n","      <td>0.000000</td>\n","      <td>0.316228</td>\n","      <td>0.316228</td>\n","      <td>0.316228</td>\n","      <td>0.316228</td>\n","      <td>0.316228</td>\n","      <td>0.316228</td>\n","      <td>0.316228</td>\n","      <td>0.000000</td>\n","      <td>0.316228</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.316228</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.242536</td>\n","      <td>0.000000</td>\n","      <td>0.242536</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.242536</td>\n","      <td>0.000000</td>\n","      <td>0.242536</td>\n","      <td>0.242536</td>\n","      <td>0.242536</td>\n","      <td>0.727607</td>\n","      <td>0.242536</td>\n","      <td>0.242536</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     across       are      away  ...  universe     while     words\n","0  0.000000  0.316228  0.000000  ...  0.000000  0.000000  0.316228\n","1  0.242536  0.000000  0.242536  ...  0.242536  0.242536  0.000000\n","\n","[2 rows x 19 columns]"]},"metadata":{"tags":[]},"execution_count":21}]}]}