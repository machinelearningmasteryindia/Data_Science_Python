{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"KNN.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNrHXDCE6giVEt6piloPIAW"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"g011WXDYm2sR","colab_type":"text"},"source":["# **KNN**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0AUGxQR-tR-x","colab_type":"text"},"source":["## **Introduction**\n","\n","The k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems. It is a method preferred by many in the industry because of its ease of use and low calculation time. \n","\n","KNN is an algorithm that is considered both non-parametric and an example of lazy learning. What do these two terms mean exactly?\n"," + Non-parametric means that it makes no assumptions. The model is made up entirely from the data given to it rather than assuming its structure is normal.\n"," + Lazy learning means that the algorithm makes no generalizations. This means that there is little training involved when using this method. Because of this, all of the training data is also used in testing when using KNN.\n","\n","Let's look some of theory behind this algorithm."]},{"cell_type":"markdown","metadata":{"id":"EdkseqNftaPh","colab_type":"text"},"source":["## **Short note on distance concept**\n","\n","A number of Machine Learning Algorithms - Supervised or Unsupervised, use Distance Metrics to know the input data pattern in order to make any Data Based decision. A good distance metric helps in improving the performance of Classification, Clustering and Information Retrieval process significantly. Choosing a good distance metric becomes really important here. The distance metric helps algorithms to recognize similarities between the contents.\n","\n","*Basic Mathematics Definition(Source Wikipedia)*,\n","> Distance metric uses distance function which provides a relationship metric between each elements in the dataset.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Zk2S6YvW2Sx7","colab_type":"text"},"source":["### **Distance Function**\n","\n","So what is the distance function?\n","\n","> A distance function provides distance between the elements of a set. If the distance is zero then elements are equivalent else they are different from each other.\n","\n","A distance function is nothing but a mathematical formula used by distance metrics. The distance function can differ across different distance metrics. Let’s talk about different distance metrics.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PKv4LFWi39Y4","colab_type":"text"},"source":["### **Distance Metrics**\n","\n","There are more than 50 different distance metric, but we will revisit some of very popular ones used in Machine learning.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QvolPa8S4GPM","colab_type":"text"},"source":["#### **Euclidean Distance**\n","\n","Let's quickly revisit our most famous theorem in school days which is Pythagorus therom. As per this theorem, we can calculate distance between two points using below formula:\n","\n","$$c = \\sqrt{a^2 + b^2}$$\n","\n","The distance calculated by this formula is famously known as **Euclidean Distance**. It is very useful when our data is continuous. Euclidean is a good distance measure to use if the input variables are similar in type (e.g. all measured widths and heights).\n","\n","Suppose we know the location of two points (A and B) like here.\n","\n","![](https://www.mathsisfun.com/algebra/images/dist-2-points-a.svg)\n","\n","What is the distance between them?\n","\n","![](https://www.mathsisfun.com/algebra/images/dist-2-points-b.gif)\n","\n","We can run lines down from A, and along from B, to make a Right Angled Triangle.\n","\n","And with a little help from Pythagoras we know that:\n","\n","$$c^2 = {a^2 + b^2}$$\n","\n","Now label the coordinates of points A and B. here\n","\n","$x_A$ means the x-coordinate of point A\n","$y_A$ means the y-coordinate of point A\n","\n","Now we can solve for c (the distance between the points):\n","\n","![](https://www.mathsisfun.com/algebra/images/dist-2-points-c.gif)\n","\n","$$c^2 = {a^2 + b^2} = {(x_A - x_B)^2 + (y_A - y_B))^2}$$\n","and, hence we can calculate c as:\n","$$ c =  \\sqrt{(x_A - x_B)^2 + (y_A - y_B))^2} $$\n","\n","Now, what happen if we will have more than two dimensions, distance in this case is calculated as:\n","\n","$$ c =  \\sqrt{(x_A - x_B)^2 + (y_A - y_B))^2 + ... + (z_A - z_B)^2} $$\n","\n","**Quick example**\n","\n","Example 1 : What is the distance between point A and point B?\n","\n","![](https://www.mathsisfun.com/algebra/images/dist-2-points-d.gif)\n","\n","Let's solve it.\n","\n","We can calculate distance as :\n","\n","$$distance_{euclidean} = \\sqrt{(x_A - x_B)^2 + (y_A - y_B))^2}$$\n","$$distance_{euclidean} = \\sqrt{(9- 3)^2 + (7 - 2))^2} = \\sqrt{6^2 + 5^2} = \\sqrt{36 + 25} = \\sqrt{61} = 7.8102..$$\n","\n","> It doesn't matter what order the points are in, because squaring removes any negatives.\n","\n","Example 2 : what is the distance between the two points (8,2,6) and (3,5,7).\n","\n","![](https://www.mathsisfun.com/algebra/images/dist-2-points-3d.svg)\n","\n","As per distance formula,\n","\n","$$ distance_{euclidean} =  \\sqrt{(x_A - x_B)^2 + (y_A - y_B))^2 + ... + (z_A - z_B)^2} $$\n","\n","$$ distance_{euclidean} =  \\sqrt{(8 - 3)^2 + (2 - 5))^2 + (6 - 7)^2} = \\sqrt{(5)^2 + (-3))^2 + (-1)^2} = \\sqrt{25 + 9 + 1} = \\sqrt{35} = 5.9$$\n","\n","Let's see its implementation in python as well."]},{"cell_type":"code","metadata":{"id":"NE87ERMSaYtI","colab_type":"code","outputId":"76a98426-eb19-4452-a4db-08763e3f6617","executionInfo":{"status":"ok","timestamp":1581237922841,"user_tz":-330,"elapsed":2325,"user":{"displayName":"Shivesh Jha","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCOcob5sxhbLtDPP9IdJeirG9gSY-Oe_l6T9cnTKw=s64","userId":"01027328504624407555"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# euclidean_distance function\n","import math\n","def euclidean_distance(x,y):\n","  return math.sqrt(sum(pow(a-b,2) for a, b in zip(x,y)))\n","\n","# Example 1\n","x = [8,2,6]\n","y = [3,5,7]\n","print(euclidean_distance(x,y))\n","\n","# Example 2\n","print(euclidean_distance([0,3,4,5],[7,6,3,-1]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["5.916079783099616\n","9.746794344808963\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ifWVgHT84Jhr","colab_type":"text"},"source":["#### **Manhattan Distance**\n","\n","We use **Manhattan Distance** if we need to calculate the distance between two data points in a grid like path. Similar to **Euclidean Distance**, we can calculate distance $d$ will be calculated using an absolute sum of difference between its cartesian co-ordinates as below :\n","\n","$$d = \\sum_{i=1}^n |{x_i - y_i|}$$ which means,\n","\n","$$ d =  {|(x_A - x_B)| + |(y_A - y_B)| + ... + |(z_A - z_B)|} $$\n","\n","It is advisable to use it when dealing with high dimensional data. Also, if you are calculating errors, it is useful when you want to emphasis on outliers due to its linear nature. Manhattan distance is a good measure to use if the input variables are not similar in type (such as age, gender, height, etc.)\n","\n","Let's calculate **Manhattan distance** for the second example using above formula:\n","\n","$$ d_{Manhattan} =  {|8 - 3| + |2 - 5| + |6 - 7|} = {5 + 3 + 1} = 9$$\n","\n"]},{"cell_type":"code","metadata":{"id":"g2FgmkDUcERS","colab_type":"code","outputId":"032b4b45-c77a-428f-bf87-1223ce1b58a5","executionInfo":{"status":"ok","timestamp":1581238968718,"user_tz":-330,"elapsed":1365,"user":{"displayName":"Shivesh Jha","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCOcob5sxhbLtDPP9IdJeirG9gSY-Oe_l6T9cnTKw=s64","userId":"01027328504624407555"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Manhattan_distance function\n","import math\n","def manhattan_distance(x,y):\n","  return sum(abs(a-b) for a, b in zip(x,y))\n","\n","# Example 1\n","x = [8,2,6]\n","y = [3,5,7]\n","print(manhattan_distance(x,y))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["9\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BgwdfXeu6Kox","colab_type":"text"},"source":["#### **Mahalanobis Distance**\n","\n","Mahalanobis Distance is used for calculating the distance between two data points in a multivariate space.\n","\n","We talked before about the Cartesian coordinate system. We drew perpendicular lines. Then we calculated distances according to that axis-system.\n","\n","This is very easy to do if our variables are not correlated. Because the distances can be measured with a straight line.\n","\n","Let’s say that two or more correlated variables are present. We will also add that we are working with more than 3 dimensions. Now, the problem gets complicated. In such cases, we can use Mahalanobis Distance. It measures distance relative to the centroid for the multivariate data. In this point, means from all variables intersect.\n","\n","According to Wikipedia Definition,\n","> The Mahalanobis distance is a measure of the distance between a point P and a distribution D. The idea of measuring is, how many standard deviations away P is from the mean of D.\n","\n","The benefit of using mahalanobis distance is, it takes covariance in account which helps in measuring the strength/similarity between two different data objects. The distance between an observation and the mean can be calculated as below -\n","\n","$$D_M(\\overrightarrow{x}) = \\sqrt{(\\overrightarrow{x} - \\overrightarrow{\\mu})^TS^{-1}(\\overrightarrow{x} - \\overrightarrow{\\mu})}$$"]},{"cell_type":"markdown","metadata":{"id":"eYDXXq188AFj","colab_type":"text"},"source":["#### **Cosine similarity**\n","\n","Mostly Cosine distance metric is used to find similarities between different documents. In cosine metric we measure the degree of angle between two documents/vectors(the term frequencies in different documents collected as metrics). This particular metric is used when the magnitude between vectors does not matter but the orientation.\n","\n","Cosine similarity formula can be derived from the equation of dot products :-\n","\n","$$similarity_{cosine} = \\cos\\theta = \\frac{A.B}{||A||||B||} = \\frac{\\sum_{i=1}^nA_iB_i}{\\sqrt{\\sum_{i=1}^n{A_i^2}}\\sqrt{\\sum_{i=1}^n{B_i^2}}}$$\n","\n","Since we know; <br>\n","$\\cos 0^\\circ = 1$ <br>\n","$\\cos 90^\\circ = 0$ <br>\n","$\\cos 180^\\circ = -1$ <br>\n","\n","Now that we have the values which will be considered in order to measure the similarities, we need to know what do 1, 0 and -1 signify. Here cosine value $1$ is for vectors pointing in the same direction i.e. there are similarities between the documents/data points. At $0$ for orthogonal vectors i.e. Unrelated(some similarity found). Value $-1$ for vectors pointing in opposite directions(No similarity).\n","\n"," + Mostly used in high dimensional positive spaces where output is bounded in the range of [0,1].\n"," + Used in Information Retrieval, Text Mining, Recommendation Systems and Classifications. It is also used to measure cohesion between the clusters in Data Mining.\n"," + It is very efficient to evaluate sparse vectors(as only the non-zero dimensions in the space are considered)\n","\n","Let's understand **Cosine similarity** in detail by solving one example. We will calculate **Cosine similarity** for the second example using above formula:\n","\n","$$ d_{cosine} =  \\frac{8*3 + 2*5 + 6*7}{\\sqrt{8^2 + 2^2 + 6^2}*\\sqrt{3^2 + 5^2 + 7^2}} = \\frac{24 + 10 + 42}{\\sqrt{64 + 4 + 36}*\\sqrt{9 + 25 + 49}} = \\frac{76}{\\sqrt{104}*\\sqrt{83}} = \\frac{76}{10.2 * 9.1} = \\frac{76}{92.81} = 0.82$$\n","\n"]},{"cell_type":"code","metadata":{"id":"k__WRj66d6DE","colab_type":"code","outputId":"690b2bd0-f8aa-4e6c-80c5-0564fd4c86e2","executionInfo":{"status":"ok","timestamp":1581242032783,"user_tz":-330,"elapsed":1051,"user":{"displayName":"Shivesh Jha","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCOcob5sxhbLtDPP9IdJeirG9gSY-Oe_l6T9cnTKw=s64","userId":"01027328504624407555"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["import numpy as np\n","def cos_sim(a,b):\n","  dot_product = np.dot(a,b)\n","  norm_a = np.linalg.norm(a)\n","  norm_b = np.linalg.norm(b)\n","  return dot_product/(norm_a*norm_b)\n","\n","# Example 1\n","x = [8,2,6]\n","y = [3,5,7]\n","print(cos_sim(x,y))\n","\n","# Example 2\n","x = [3,45,7,2]\n","y = [2,54,13, 15]\n","print(cos_sim(x,y))\n","\n","# Example 2\n","x = [1, 2]\n","y = [100, 200]\n","print(cos_sim(x,y))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.8180086129282734\n","0.9722842517123499\n","1.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xHpBtE-reBQq","colab_type":"text"},"source":["**Disadvantage :** Cosine similarity is subjective to the domain and application and is not an actual distance metric. For example data points $[1,2]$ and $[100,200]$, are shown similar with cosine similarity, whereas in eucildean distance measure shows they are far away from each other (in a way not similar).\n","\n","**Situation #1 :** If we want to find out closest merchants (like costco, walmart, delta, starbugs, etc) to a given merchant, we would go for cosine similarity because we want merchants in similar domains (i.e, orientation) rather than the magnitude of the feature vector.\n","\n","**Situation #2 :** If we want to find out closest customers to a given customer, we would go for euclidean distance because we want customers who are close in magnitude to the current customer."]},{"cell_type":"markdown","metadata":{"id":"pfJPo1weRwcR","colab_type":"text"},"source":["#### **Jaccard Index**\n","\n","The **Jaccard Index** also known as Jaccard similarity coefficient (originally given the French name *coefficient de communauté* by **Paul Jaccard**), is a statistic used for gauging the similarity and diversity of sample sets. The Jaccard coefficient measures similarity between finite sample sets, and is defined as the size of the intersection divided by the size of the union of the sample sets:\n","\n","$$J(A,B) = \\frac{|A \\cap B|}{|A \\cup B|} = \\frac{|A \\cap B|}{|A| + |B| - |A \\cap B|}$$\n","\n","(If A and B are both empty, define J(A,B) = 1) $0\\le J(A,B)\\le 1$\n","\n","The **Jaccard distance**, which measures dissimilarity between sample sets, is complementary to the Jaccard coefficient and is obtained by subtracting the Jaccard coefficient from 1, or, equivalently, by dividing the difference of the sizes of the union and the intersection of two sets by the size of the union:\n","\n","$$d_{J}(A,B) =1-J(A,B) = \\frac{|A\\cup B|-|A\\cap B|}{|A\\cup B|}$$\n","\n","Let's see some practical example.\n","\n","**Example :**\n","Given two sets of integers s1 and s2, let's find the Jaccard Index and the Jaccard Distance between the two sets.\n","\n","$s1 = {1, 2, 3, 4, 5}$\n","\n","$s2 = {4, 5, 6, 7, 8}$\n","\n","Answer: Let's solve this by using above formula:\n","\n","${|A \\cap B|} = 2 $ (Since only two elements 4 and 5 are common in both sets.)<br>\n","$|A| = 5$ (number of elements in set 1)<br>\n","$|B| = 5$ (number of elements in set 2)\n","\n","hence, Jaccard index can be calculated as:\n","$$J(A,B) = \\frac{|A \\cap B|}{|A| + |B| - |A \\cap B|} = \\frac{2}{5 + 5 - 2} = \\frac{2}{8} = 0.25$$\n","\n","also, Jaccard distance can be calculated as:\n","$$d_{J}(A,B) =1-J(A,B) = 1 - 0.25 = 0.75$$\n","\n","There are various other distance metric depend on its use cases. Some of distance metric implemented in `sklearn` library of **python** are given [here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html).\n"]},{"cell_type":"markdown","metadata":{"id":"nBiIPGozMzix","colab_type":"text"},"source":["## **K-Nearest Neighbors**\n","\n","The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other.\n","\n","> “Birds of a feather flock together.”\n"]},{"cell_type":"markdown","metadata":{"id":"R46kvo0MNMNs","colab_type":"text"},"source":["### **Assumptions in KNN**\n","\n","Before using KNN, let us revisit some of the assumptions in KNN.\n","\n","1. KNN assumes that the data is in a feature space. More exactly, the data points are in a metric space. The data can be scalars or possibly even multidimensional vectors. Since the points are in feature space, they have a notion of distance – This need not necessarily be **Euclidean distance** although it is the one commonly used.\n","\n","2. Each of the training data consists of a set of vectors and class label associated with each vector. In the simplest case , it will be either + or – (for positive or negative classes). But KNN , can work equally well with arbitrary number of classes.\n","\n","3. We are also given a single number \"k\" . This number decides how many neighbors (where neighbors is defined based on the distance metric) influence the classification. This is usually a odd number if the number of classes is 2. If k=1 , then the algorithm is simply called the nearest neighbor algorithm.\n","\n","> Since KNN is based on distance measure, it is very important to remove scale from each features for optimum distance calculation. For this, we need to perform standardization beforehand implementing KNN."]},{"cell_type":"markdown","metadata":{"id":"mSMtSIHQkOoG","colab_type":"text"},"source":["### **How KNN works?**\n","\n","KNN Algorithm works in following way. \n","\n","1. Load the training data.\n","2. Initialize K to your chosen number of neighbors.(default = 5 in `sklearn` implementation)\n","3. For each example in the training data, calculate the distance between the query observation from test data and the current example from the training data.\n","4. Sort the ordered collection of distances from smallest to largest (in ascending order) by the distances.\n","5. Pick the first K entries from the sorted collection.\n","6. Get the labels of the selected K entries.\n","7. If regression, return the mean of the K labels.\n","8. If classification, return the mode of the K labels."]},{"cell_type":"code","metadata":{"id":"ornqHYUbmt3P","colab_type":"code","outputId":"5685a80a-6847-4530-c91e-4bd4b7cf9099","executionInfo":{"status":"ok","timestamp":1581244441762,"user_tz":-330,"elapsed":43438,"user":{"displayName":"Shivesh Jha","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCOcob5sxhbLtDPP9IdJeirG9gSY-Oe_l6T9cnTKw=s64","userId":"01027328504624407555"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["# Mounting google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"83NiESyGg4Eo","colab_type":"code","colab":{}},"source":["# Setting the path\n","import os\n","os.chdir(\"/content/drive/My Drive/Introduction to Data Science - Python edition/dataset/titanic\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z5EjSHvkg_H-","colab_type":"code","colab":{}},"source":["# Loading libraries\n","import numpy as np\n","import pandas as pd\n","import sklearn.model_selection as ms\n","import sklearn.metrics as sklm\n","import numpy.random as nr\n","import matplotlib.pyplot as plt\n","import math\n","\n","%matplotlib inline\n","\n","pd.set_option('display.max_rows', 100)\n","pd.set_option('display.max_columns', 100)\n","pd.set_option('display.float_format', lambda x: '%.2f' % x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fArNP8mZg_3V","colab_type":"code","colab":{}},"source":["# load npy file\n","X_train = np.load('X_train.npy')\n","X_validation = np.load('X_validation.npy')\n","X_test = np.load('X_test.npy')\n","\n","y_train = np.load('y_train.npy')\n","y_validation = np.load('y_validation.npy')\n","y_test = np.load('y_test.npy')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ijgso5Q9hAg-","colab_type":"code","outputId":"fb019683-4e99-481d-b2ac-1f97b3968bd1","executionInfo":{"status":"ok","timestamp":1581245228559,"user_tz":-330,"elapsed":1638,"user":{"displayName":"Shivesh Jha","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCOcob5sxhbLtDPP9IdJeirG9gSY-Oe_l6T9cnTKw=s64","userId":"01027328504624407555"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["from sklearn.neighbors import KNeighborsClassifier\n","KNN = KNeighborsClassifier(n_neighbors=5)\n","KNN = KNN.fit(X_train, y_train)\n","KNN"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n","                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n","                     weights='uniform')"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"8OAJErCTg_Ei","colab_type":"code","outputId":"441ef395-73f6-4c85-c266-3f8457ad863e","executionInfo":{"status":"ok","timestamp":1581245233080,"user_tz":-330,"elapsed":1465,"user":{"displayName":"Shivesh Jha","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCOcob5sxhbLtDPP9IdJeirG9gSY-Oe_l6T9cnTKw=s64","userId":"01027328504624407555"}},"colab":{"base_uri":"https://localhost:8080/","height":550}},"source":["def score_model(probs, threshold):\n","    return np.array([1 if x > threshold else 0 for x in probs[:,1]])\n","\n","def print_metrics(labels, probs, threshold):\n","    scores = score_model(probs, threshold)\n","    metrics = sklm.precision_recall_fscore_support(labels, scores)\n","    conf = sklm.confusion_matrix(labels, scores)\n","    print('                 Confusion matrix')\n","    print('                 Score positive    Score negative')\n","    print('Actual positive    %6d' % conf[0,0] + '             %5d' % conf[0,1])\n","    print('Actual negative    %6d' % conf[1,0] + '             %5d' % conf[1,1])\n","    print('')\n","    print('Accuracy        %0.2f' % sklm.accuracy_score(labels, scores))\n","    print('AUC             %0.2f' % sklm.roc_auc_score(labels, probs[:,1]))\n","    print('Macro precision %0.2f' % float((float(metrics[0][0]) + float(metrics[0][1]))/2.0))\n","    print('Macro recall    %0.2f' % float((float(metrics[1][0]) + float(metrics[1][1]))/2.0))\n","    print(' ')\n","    print('           Positive      Negative')\n","    print('Num case   %6d' % metrics[3][0] + '        %6d' % metrics[3][1])\n","    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])\n","    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])\n","    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])\n","\n","def plot_auc(labels, probs):\n","    ## Compute the false positive rate, true positive rate\n","    ## and threshold along with the AUC\n","    fpr, tpr, threshold = sklm.roc_curve(labels, probs[:,1])\n","    auc = sklm.auc(fpr, tpr)\n","    \n","    ## Plot the result\n","    plt.title('Receiver Operating Characteristic')\n","    plt.plot(fpr, tpr, color = 'orange', label = 'AUC = %0.2f' % auc)\n","    plt.legend(loc = 'lower right')\n","    plt.plot([0, 1], [0, 1],'r--')\n","    plt.xlim([0, 1])\n","    plt.ylim([0, 1])\n","    plt.ylabel('True Positive Rate')\n","    plt.xlabel('False Positive Rate')\n","    plt.show()    \n","\n","probabilities = KNN.predict_proba(X_validation)\n","print_metrics(y_validation, probabilities, 0.5)  \n","plot_auc(y_validation, probabilities)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["                 Confusion matrix\n","                 Score positive    Score negative\n","Actual positive        72                10\n","Actual negative        15                37\n","\n","Accuracy        0.81\n","AUC             0.86\n","Macro precision 0.81\n","Macro recall    0.79\n"," \n","           Positive      Negative\n","Num case       82            52\n","Precision    0.83          0.79\n","Recall       0.88          0.71\n","F1           0.85          0.75\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dd3hUZfbA8e9JKKGDIAqEpqAQkBop\nFrALWPAnSrFiw95AF13dteHadXVtCGJZV1y7qFhWpQhKld6bQpASIPQEUs7vj/eGDCFlEjJzZybn\n8zzzMDP3ztwzl8k9877vvecVVcUYY4wpTJzfARhjjIlsliiMMcYUyRKFMcaYIlmiMMYYUyRLFMYY\nY4pkicIYY0yRLFGYoInI5SLyvd9xRBIR2S0ix/iw3WYioiJSIdzbDgURWSQip5XidfadDANLFFFK\nRH4XkXTvQLVRRN4Wkeqh3Kaq/kdVzwnlNgKJyEki8pOI7BKRHSLypYgkhWv7BcQzUUSuD3xOVaur\n6uoQbe84EflIRLZ4n3++iAwVkfhQbK+0vITV4nDeQ1XbqOrEYrZzSHIM93eyvLJEEd0uUNXqQAeg\nI3C/z/GUSkG/ikWkO/A98AXQEGgOzAOmhuIXfKT9MheRY4HpwDrgBFWtBVwKJAM1ynhbvn32SNvv\nphCqarcovAG/A2cFPH4a+DrgcWXgWWAtsAl4HagSsLwvMBfYCawCennP1wLeBDYA64ERQLy3bDAw\nxbv/GvBsvpi+AIZ69xsCnwCpwBrgjoD1HgY+Bt7ztn99AZ/vZ+DVAp7/BnjXu38akAL8Fdji7ZPL\ng9kHAa8dDmwE/g3UAb7yYk7z7id66z8OZAMZwG7gZe95BVp4998GXgG+BnbhDvTHBsRzDrAM2AG8\nCkwq6LN7674X+P9ZwPJm3rav9j7fFuCBgOVdgF+B7d7/5ctApYDlCtwKrADWeM+9iEtMO4HZwKkB\n68d7+3mV99lmA42Byd577fH2ywBv/fNx36/twC9Au3zf3eHAfGAfUIGA77MX+ywvjk3A897za71t\n7fZu3Qn4TnrrtAH+B2zzXvtXv/9WY+HmewB2K+V/3MF/WInAAuDFgOUvAOOAI3C/QL8EnvCWdfEO\nVmfjWpWNgFbess+AkUA1oD4wA7jRW3bgjxLo4R1UxHtcB0jHJYg470Dyd6AScAywGjjXW/dhIBO4\nyFu3Sr7PVhV3UD69gM99DbDBu38akAU8j0sKPb0D1vFB7IPc1z7lvbYKUBfo522/BvAR8HnAtieS\n78DOoYliq7d/KwD/AT7wltXzDnwXe8vu9PZBYYliI3BNEf//zbxtj/Jib4876Lb2lncGunnbagYs\nAe7KF/f/vH2Tmzyv8PZBBWCYF0OCt+xe3HfseEC87dXNvw+8xx2BzUBXXIK5Gvd9rRzw3Z2LSzRV\nAp7L/T7/Clzp3a8OdMv3mSsEbGswed/JGrikOAxI8B539ftvNRZuvgdgt1L+x7k/rN24X3cK/AjU\n9pYJ7oAZ+Gu2O3m/HEcCLxTwnkd5B5vAlscgYIJ3P/CPUnC/8Hp4j28AfvLudwXW5nvv+4G3vPsP\nA5OL+GyJ3mdqVcCyXkCmd/803MG+WsDyD4G/BbEPTgP25x4IC4mjA5AW8HgixSeK0QHL+gBLvftX\nAb8GLBNcoi0sUWTitfIKWZ570EwMeG4GMLCQ9e8CPssX9xnFfMfSgPbe/WVA30LWy58oXgMey7fO\nMqBnwHf32gK+z7mJYjLwCFCvkM9cWKIYBMwJ5d9deb1Z/2B0u0hVfxCRnsD7uF+t24Ejcb+KZ4tI\n7rqC+3UH7pfc+ALerylQEdgQ8Lo43AHtIKqqIvIB7o9zMnAZrrsk930aisj2gJfE47qTch3yngHS\ngBygAbA037IGuG6WA+uq6p6Ax3/gWjXF7QOAVFXNOLBQpCquFdIL10ICqCEi8aqaXUS8gTYG3N+L\n+0WMF9OBz+ztv5Qi3mcr7rOWansichyupZWM2w8VcK28QAf9H4jIPcB1XqwK1MR9p8B9Z1YFEQ+4\n//+rReT2gOcqee9b4LbzuQ54FFgqImuAR1T1qyC2W5IYTQnYYHYMUNVJuF+zz3pPbcF1A7VR1dre\nrZa6gW9wf6THFvBW63AtinoBr6upqm0K2fRY4BIRaYprRXwS8D5rAt6jtqrWUNU+gWEX8Xn24Lof\nLi1gcX9c6ylXHRGpFvC4CfBnEPugoBiG4bpWuqpqTVz3GrgEU2TMQdiAaym5N3TZK7Hw1fkB1w1W\nWq/hkmxL77P8lbzPkevA5xGRU4G/4PZvHVWtjeuezH1NYd+ZgqwDHs/3/19VVccWtO38VHWFqg7C\ndX0+BXzs/R8Xt//X4bo5TRmzRBE7/gmcLSLtVTUH13f9gojUBxCRRiJyrrfum8A1InKmiMR5y1qp\n6gbcmUbPiUhNb9mxXovlEKo6B3dAHg18p6q5LYgZwC4RGS4iVUQkXkTaisiJJfg89+F+ld4hIjVE\npI6IjMB1Hz2Sb91HRKSSd7A7H/goiH1QkBq45LJdRI4AHsq3fBOlPxB9DZwgIhd5Z/rcChxdxPoP\nASeJyDMicrQXfwsReU9EagexvRq4MZHdItIKuDmI9bNwA/kVROTvuBZFrtHAYyLSUpx2IlLXW5Z/\nv4wCbhKRrt661UTkPBEJ6mwtEblCRI70/g9zv1M5Xmw5FP5/8BXQQETuEpHK3vemazDbNEWzRBEj\nVDUVeBc3gAzurJKVwDQR2Yn7hXq8t+4M3KDwC7hfjZNw3QXg+tIrAYtxXUAfU3QXyPvAWd6/ubFk\n4w7YHXBnPOUmk1ol+DxTgHNxg78bcF1KHYFTVHVFwKobvTj/xA0e36Squd1Vhe6DQvwTNzC8BZgG\nfJtv+Yu4FlSaiLwU7GfxPs8WXAvpaVy3UhLuzJ59hay/CpcUmwGLRGQHrsU2CzcuVZx7cN2Bu3AH\n7v8Ws/53uM+7HLevMzi4e+h53PjP97gE9CZuX4Ebc3pHRLaLSH9VnYUbs3oZ93+zEjeWEKxeuM+8\nG7fPB6pquqruxZ19NtXbVrfAF6nqLtwJGhfgvhcrgNNLsF1TiNwzVoyJOt6VvO+palFdOBFJROJw\np+derqoT/I7HmKJYi8KYMBGRc0WktohUJm/MYJrPYRlTrJAlChEZIyKbRWRhIctFRF4SkZVeaYJO\noYrFmAjRHXdWzhZc98hFqprub0jGFC9kXU8i0gN3nv+7qtq2gOV9gNtx55p3xV0sZgNPxhgTYULW\nolDVybjL6AvTF5dEVFWnAbVFJJjzxo0xxoSRnxfcNeLgsypSvOc25F9RRIYAQwCqVavWuVWrVmEJ\n0BhjIo5mQU7mwTfNvb8/73Fub9EWYC/MzmaLqh5Zmk1GxZXZqvoG8AZAcnKyzpo1y+eIjDGmDGkO\n7NsK6Ru825+QEXD/wL8bIaeAM6or1oQqDSChAVRpCAlHu8dVG8F/Z8COLOSJl/8obXh+Jor1uEvu\ncyV6zxljTGzQHMhILeSgH5gUNrqWQH4Va7sDfpWGcOSpeferNDj4foWA4gTr18PNN8OATnD5IBg+\nyD3/xMul/hh+JopxwG1evaCuwA7vymBjjIlsOdmwb3PBB/3AfzM2ua6i/CodkXegr3l8wQf/hAZQ\nocqhry2MKoweDffcA5mZcN55ZfZxQ5YoRGQsrkJnPa/42UO4gnOo6uu4onR9cFdt7sVdKWyMMf7J\nyXIH9wIP/gFdQhmbXGshv8r18g70tdt63UANoKr3b5UGUOVoiE8o27hXrYIbboAJE+D002HUKDg2\n2NJcxQtZovCKehW1PHfiFGOMCa3s/fkSQO4v/g2wN2A8IGMzh9YeFEion/cr/4iOAQf9hnn/JhwF\n8ZX8+HSwYAHMng1vvAHXXw+Sv/7j4YmKwWxjjClQ9r68X/sHHfTztQL2bTn0tRLnDu4JDaBKIhxx\nYsFjAAn1Ia5i+D9bcRYuhN9+g6uugosugtWroW7d4l9XCpYojDGRJyu9gAHgAsYA9hdwqZbEe2f9\nNIRqzaBe94LHACrXh7j4Q18f6fbvh3/8w92OOgr694eEhJAlCbBEYYwJp6w9hR/0A+9nbj/0tXEV\n8xJAjZZQv2fBA8AJR7rWQiyaPh2uuw4WLYIrroAXXnBJIsQsURhjDl/mrqJ//ee2DjJ3HvrauEp5\nB/pareHoMwO6fQISQeUjYjcBBGP9ejj1VNeK+OqrMj2rqTiWKIwxBVN1B/aCDv4Z+RJB1p5DXx+f\nEHAGUDs4+tyCxwAq1SnzwdeYsnw5HHccNGoE//0vnHkm1KxZ/OvKkCUKY8obVde1kzvwG3jWT/6k\nkF1Acdv4qu4AX7Uh1OkEDc8reAygYi1LAIdj+3b4y1/ctRETJ0KPHvB//+dLKJYojIlVqrBnDWyb\nA2nebccSlwAKKgNRoUbegb5ul4IP/lUauPUsAYTWuHHu6uqNG+Hee+HEkswiXPYsURgTC3KyYOdS\nlwy2zYG03yBtLmTucMslHmq2hnrdoGrioQPAVRpAxer+fgbjXH89vPkmnHACfPEFJCf7HZElCmOi\nTlY6bF/gtRJ+c4lhxwLIznDL4xOgdntoOshdHFanI9RqW7JyECa8ciu9irjE0LQpDB8OlXy6gC8f\nSxTGRLL9213LYNtved1HO5eCZrvlFWu7ZNDyFpcQ6nR0tYPi7E87aqxbBzfdBAMHwpVXuvsRxr5N\nxkSK9A0HJ4Rtc9wYQ64qDV0iSPy/vJZCtWY2XhCtcnJg5EjXcsjO9m2gOhiWKIwJN82B3asDxhO8\nW8amvHWqt4C6ydDihryWQpWj/IvZlK0VK9xYxOTJcNZZrkZT8+Z+R1UoSxTGhFJOpjvTKC0gIaTN\nzbvwTCpArSRo0AuO6OQlhfZuIhoTuxYvhvnzYcwYGDw44luFliiMKStZe2H7/IPPPNq+MO9U1Piq\nLgk0u8IlhCM6Qq02ZV9y2kSmefNg7ly4+mro29cV8atTx++ogmKJwpjS2LfNtQxyzzpKmwO7luXN\nUVDpCJcMjr89r+uoxnHRWYTOHJ59+2DECHjySWjQAAYMcPWZoiRJgCUKY4qm6i5QCxxkTpsDewKm\nH66a6BJBk0vzWgpVm0R8d4IJg19/dUX8lixx5cCffz4sRfzKmiUKY3JpDuxaefBZR2lzYF+qt4K4\nqqV1u0HLm135ijodXLVSY/Jbvx569oSjj4bx46F3b78jKjVLFKZ8yt4POxcffNZR2lzI2u2Wx1V0\n4weNzncJ4YiOrrBdxRr+xm0i35Il0Lq1K+L34YeuiF+N6P7eWKIwsS9rD6TNO7ilsGMh5Ox3yytU\nc1cyHzM4bzyhVhv/prU00SktDYYNg7fecqe9nnqqm3kuBliiMLFl39Z81yf8BjuXc2Ae5Mr1vEHm\nu/LGE6q3sEFmc3g++wxuuQVSU+H++30v4lfWLFGY6KQKe1O8pBAw0Lx3Xd46VZu4RNB0UF5LoWqi\nDTKbsnXtta4V0aEDfP01dOrkd0RlzhKFiXw52bBrRb6L1ua41gMA4uobHXlKXiuhTkeoHLo5hE05\nF1jEr1s3aNkS7rkHKlb0N64QsURhIkv2Ptix6ODuo+3z8mZQi6vkKqEmXuSdddQR6rRz4wzGhMMf\nf8CNN8Jll7lTXocM8TuikLNEYfyTuevgQea0OS5J5GS65RWqu0RwzHV5rYSarW2Q2fgjJwdeew3u\nu8+1KC691O+IwsYShQmPjNR81yf85q5ZODDIfKSrddSgV8CVzMeCxPkatjEALFvmivhNmQLnnOOq\nvjZr5ndUYWOJwpQtVdi7Nq/bKHegOX193jrVmrlE0OzKvJZClYY2yGwi17JlsGgRvP22624qZ99V\nSxSm9HKyYdfyfGcezYX929xyiYOareCo0/JaCXU6QOUjfA3bmKDMmeOK+F1zDVx4oSviV7u231H5\nwhKFCU72PneRWm5C2DbHVUrN3uuWx1WG2idA43555bJrnwAVqvobtzEllZEBjz4KTz/trq4eNMjV\nZyqnSQIsUZiCZO70pt8MHGReDJrllles6VoGLYbknY5as5Ure2FMNJs61RXxW7bMtSSeey4qi/iV\nNUsU5V36poPPOto2B3avzFuecLRLBo3Oz+s+qt7cBplN7Fm/Hk4/3bUivvvODVobwBJF+aEKe34/\ntLxF+oa8daof452OOjivpVClgV8RGxMeixdDUpJLEJ984pJF9ep+RxVRLFHEuh2LYc69kPoLZG53\nz0m8ux7hqLO8s446uZnXKpXfPlhTDm3bBkOHwjvvwKRJ0KMHXHCB31FFJEsUsSonG5a9APMedKWx\nmw7IG2Su1RYqVPE7QmP888kncOutsHUrPPAAdOnid0QRzRJFLNq9Gn4dDKk/Q2JfOHEkVDnK76iM\niQyDB7tWRKdO8O23rpifKZIliliiCivfgDnDXPdSt3eg+ZXl7uIgYw4RWMTvpJPcxELDhkEFOwQG\nI6SnrohILxFZJiIrReS+ApY3EZEJIjJHROaLSJ9QxhPT9q6Hib1h5k1Qrzv0WQjHlL8rSI05xJo1\n7gymd991j4cMgeHDLUmUQMgShYjEA68AvYEkYJCIJOVb7UHgQ1XtCAwEXg1VPDFLFdb8B75uC5t/\nhuRX4PTvoFpjvyMzxl/Z2fDSS9C2LUyblteqMCUWypTaBVipqqsBROQDoC+wOGAdBWp692sBf4Yw\nntiTkQozb4Z1n0C9k6Db21Czpd9RGeO/JUvchXO//gq9e8Prr0OTJn5HFbVCmSgaAQHTjZECdM23\nzsPA9yJyO1ANOKugNxKRIcAQgCb2n+2kjIMZN8D+7dDhSWh1j03naUyulSvd1dX//jdcfrl1wR4m\nvy+vHQS8raqJQB/g3yKHXvKrqm+oarKqJh955JFhDzKi7N8B066ByX1dxdVesyBpuCUJY2bPhjFj\n3P0LLnBjE1dcYUmiDIQyUawHAjvKE73nAl0HfAigqr8CCUC9EMYU3Tb+CONPgDX/hjYPwjnTXeE9\nY8qz9HQ3mVDXrvDYY66oH0DNmkW/zgQtlIliJtBSRJqLSCXcYPW4fOusBc4EEJHWuESRGsKYolPW\nXph1O/x0lqvGevYv0P4xm+nNmMmToX17eOopd33EnDlWxC8EQjZGoapZInIb8B0QD4xR1UUi8igw\nS1XHAcOAUSJyN25ge7CqnZpwkNRfYdrVsGsFHH8ntH/Crqo2BlwRvzPPhMaN4Ycf3H0TEhJtx+Xk\n5GSdNWuW32GEXvY+WPAwLHkaqjaGbm/BUaf7HZUx/luwAE7wuly/+soV8atWzd+YooCIzFbV5NK8\n1u/BbFOQtHnwXRdY/CQccw30mW9JwpgtW+DKK6FdO9flBHD++ZYkwsAuTYwkOVmuBbHgYahUF3p+\n6eaBMKY8U4WPPoLbboO0NHjoITdwbcLGEkWk2LkMfr0atk6HJv3hxFehcl2/ozLGf1df7a6HSE6G\nH3/M63YyYWOJwm+aA8tfhrn3QXwVOPkDVxLcmPIssIhfz56uu+muu6w+k09sr/tpzx/u4rlNE6Bh\nH+g62maUM2b1arjhBnex3DXXuFIcxlc2mO0HVVg1Br4+AbbOhC6joOdXliRM+ZadDf/8p+tamjkT\n4uzwFCmsRRFu6Rth+g3w51dQv6cr5Fe9md9RGeOvxYvh2mth+nQ47zxXxC8x0e+ojMcSRTit/chV\ne83aA51egOPvgENLWxlT/qxZA6tWwfvvw8CBVp8pwliiCId922DWbfDHWDjiROj+LtRq5XdUxvhr\n5kyYO9eNR5x3nhubqFHD76hMAeznbKj9+Q2Mb+taE+0eg3N+sSRhyre9e+Gee6BbN3jiibwifpYk\nIpYlilDJ3AXTh8DEPu7iuXNnQNsHIc4acaYcmzjRner63HOuJWFF/KKCHbVCYfNk+HUw7PkdWv8F\n2j0K8ZX9jsoYf6WkwNlnQ9Om8NNPrkaTiQrWoihrC0fAD6e5Qeqzf4aOT1mSMOXbvHnu38RE+OIL\nmD/fkkSUsURRlnYuhwUPQZNLoPdcOPJkvyMyxj+pqXDZZdChA0ya5J7r0weqVvU3LlNi1vVUlhb9\nA+IqQ/LLULG639EY4w9V+OADuOMO2LEDHnkEunf3OypzGIJqUYhIJRFpEepgotru1fD7e9DiRkio\n73c0xvjnyitdS+LYY91g9d//DpVsNsZoVmyiEJHzgAXA/7zHHUTks1AHFnUWPQlSAVrf63ckxoRf\nTk5eIb/TT4fnn4epU6FNG3/jMmUimBbFo0BXYDuAqs4FrHURaM9aWPM2HHsdVG3odzTGhNfKlW4a\n0rfeco+vuw7uvhvi4/2Ny5SZYBJFpqpuz/dcdM2fGmqLn3b/Jg33Nw5jwikrC5591hXxmzPHupdi\nWDCD2UtEpD8QJyLNgTuAaaENK4qkb4BVo6H51VCtid/RGBMeCxe6EuCzZkHfvvDqq9DQWtOxKpgW\nxW1AZyAH+BTYB9wZyqCiyuJnQLOgzf1+R2JM+KxdC3/84c5u+uwzSxIxLpgWxbmqOhw40K8iIhfj\nkkb5lrEZVr4OzS6H6sf4HY0xoTV9urt4bsgQdz3E6tVQ3U4DLw+CaVE8WMBzD5R1IFFp6fOQnQFt\n/up3JMaEzp49MHSouxbi6adh3z73vCWJcqPQFoWInAv0AhqJyPMBi2riuqHKt31bYfkrbn7rmsf7\nHY0xofHTT6543+rVcPPN8OSTUNlK0pQ3RXU9bQYWAhnAooDndwH3hTKoqLDsRcjaDW2scWViVEoK\nnHsuNG/uSnD06OF3RMYnhSYKVZ0DzBGR/6hqRhhjinz7d8CylyDx/6B2W7+jMaZszZkDHTu6In5f\nfgk9e0KVKn5HZXwUzBhFIxH5QETmi8jy3FvII4tky/8FmTvc/BLGxIpNm2DAAOjUKa+IX69eliRM\nUInibeAtQIDewIfAf0MYU2TL3AVLX4CG58ERnfyOxpjDpwrvvQdJSfD55zBiBJx0kt9RmQgSTKKo\nqqrfAajqKlV9EJcwyqcVr8H+bdD2b35HYkzZuOwyV8jv+OPdHNYPPAAVK/odlYkgwVxHsU9E4oBV\nInITsB4on5PbZu2Fpc/B0WdDva5+R2NM6eXkgIi7nXOOO/X11lutPpMpUDAtiruBarjSHScDNwDX\nhjKoiLXyDXeRnbUmTDRbvtxVeB0zxj2+5ho3d4QlCVOIYlsUqjrdu7sLuBJARBqFMqiIlJ0BS56G\n+j2h/ql+R2NMyWVlufLfDz0ECQk2SG2CVmSLQkROFJGLRKSe97iNiLwLTC/qdTFp1RhXANBaEyYa\nzZ8P3brB8OHQuzcsXuzGJowJQqGJQkSeAP4DXA58KyIPAxOAecBxYYkuUmTvh8VPQr3ucNQZfkdj\nTMmlpMC6dfDRR/DJJ9Cggd8RmShSVNdTX6C9qqaLyBHAOuAEVV0d7JuLSC/gRSAeGK2qTxawTn/g\nYdwcF/NUNfJ+5qx5F/augy4j3eCfMdHgl19cS+Kmm/KK+FWr5ndUJgoV1fWUoarpAKq6DVhewiQR\nD7yCO5U2CRgkIkn51mkJ3A+crKptgLtKGH/o5WTB4ifgiGRo0MvvaIwp3u7dcOedcMop8NxzeUX8\nLEmYUiqqRXGMiOSWEhegecBjVPXiYt67C7AyN7mIyAe4VsrigHVuAF5R1TTvPTeXMP7Q+/192L0a\nejxvrQkT+b7/3pUBX7vWne76j39YET9z2IpKFP3yPX65hO/dCNddlSsFN/d2oOMARGQqrnvqYVX9\nNv8bicgQYAhAkyZhnEUuJxsWPQ6120GjC8O3XWNKY906OO88OPZYmDzZtSiMKQNFFQX8MUzbbwmc\nBiQCk0XkhPxzdKvqG8AbAMnJyeGbr3vtR7BrOZzyobUmTOSaPRs6d4bGjWH8eDj1VHf6qzFlJJgL\n7kprPdA44HGi91ygFGCcqmaq6hpgOS5x+E9zYNEIqNkaGudvXBkTATZuhEsvheTkvCJ+Z59tScKU\nuVAmiplASxFpLiKVgIHAuHzrfI5rTeBdq3EcEPSAeUilfA47Frn5JiSUu8mYElKFd95xRfy+/NKN\nQ1gRPxNCwdR6AkBEKqvqvmDXV9UsEbkN+A43/jBGVReJyKPALFUd5y07R0QWA9nAvaq6tWQfIQRU\nYeEIqN7CzWBnTCQZOBA+/BBOPhlGj4ZWrfyOyMS4YhOFiHQB3gRqAU1EpD1wvareXtxrVXU8MD7f\nc38PuK/AUO8WOf78GtLmQNcxEBd0LjUmdAKL+PXp48YhbrkF4qy1a0IvmG/ZS8D5wFYAVZ0HnB7K\noHylCgsfg2rNoPkVfkdjDCxd6qYhffNN9/jqq+G22yxJmLAJ5psWp6p/5HsuOxTBRISN/4OtMyDp\nPoizmvzGR5mZbvyhfXtXm6l6db8jMuVUMP0q67zuJ/Wutr4dd3ZS7MltTVRNhGMG+x2NKc/mznXl\nv+fOhUsugX/9C44+2u+oTDkVTKK4Gdf91ATYBPzgPRd7Nk+C1CnQ+SWIt6tZjY82bnS3Tz6Bi4sr\ngmBMaAWTKLJUdWDII4kECx+DhKPg2Ov9jsSUR1OmuCJ+t9wCvXrBqlVQtarfURkT1BjFTBEZLyJX\ni0jsToGa+gts+gla3wsVbEIXE0a7drnB6VNPhX/+M6+InyUJEyGKTRSqeiwwAugMLBCRz0Uk9loY\nCx+DyvWg5U1+R2LKk+++g7Zt4dVXXcXX336zIn4m4gR1fp2q/qKqdwCdgJ24CY1ix9aZsOFbaDUU\nKlgpZhMm69bB+ee7lsOUKa41YWc2mQhUbKIQkeoicrmIfAnMAFKB2KoXsHAEVKoDx93qdyQm1qnC\njBnufuPG8M03MGeOleAwES2YFsVCoBvwtKq2UNVhqho7c2anzYP14+D4O6FiTb+jMbFswwbo1w+6\nds0r4nfWWVbEz0S8YM56OkZVc0IeiV8WPQ4VasDxd/gdiYlVqvD22zB0KGRkwFNPuTpNxkSJQhOF\niDynqsOAT0TkkDkggpjhLvLtWAJrP4Y297uuJ2NCoX9/+Phjd1bT6NFw3HF+R2RMiRTVoviv929J\nZ7aLHosehwpV4fi7/Y7ExJrsbFfALy4OLrgAzjgDbrzR6jOZqFTot1ZVvRE3Wqvqj4E3oHV4wguh\nnSvgj7HQ8mZIqOd3NCaWLF1rZl0AAB1gSURBVFniWg+5RfyuugpuvtmShIlawXxzry3guevKOpCw\nW/wExFWCVsP8jsTEisxMGDECOnSAZcugVi2/IzKmTBQ1RjEANytdcxH5NGBRDWB7wa+KErt/hzX/\nhpa3QBUrtGbKwJw5MHiwK8ExYAC89BLUr+93VMaUiaLGKGbg5qBIBF4JeH4XMCeUQYXc4ifd9KZJ\nf/E7EhMrNm2CLVvg88+hb1+/ozGmTBWaKFR1DbAGVy02duxNgdVvwTHXQtVGfkdjotnkybBgAdx6\nqyvit3IlVLE6YSb2FDpGISKTvH/TRGRbwC1NRLaFL8Qytvhp0Bxoc5/fkZhotXOnq/Das6frYsot\n4mdJwsSoogazc6c7rQccGXDLfRx90jfCqlHQ/Cqo1tTvaEw0Gj8e2rSBkSPdBXRWxM+UA0WdHpt7\nNXZjIF5Vs4HuwI1AdFbOW/Is5Ox3F9gZU1Lr1rnxh1q14Jdf4LnnoFp0/ikYUxLBnB77OW4a1GOB\nt4CWwPshjSoUMlJhxWvQdBDUaOF3NCZaqMK0ae5+48bw/feuFdG1q79xGRNGwSSKHFXNBC4G/qWq\ndwPRNwq89AXIToc2D/gdiYkWf/4JF10E3bvnFfE7/XSoVMnfuIwJs2ASRZaIXApcCXzlPVcxdCGF\nwL5tsPxlaHIJ1Ir+i8pNiKm6mkxJSa4F8eyzVsTPlGvBVI+9FrgFV2Z8tYg0B8aGNqwytuwlyNoF\nbR70OxITDS65BD791J3VNHo0tLCuSlO+FZsoVHWhiNwBtBCRVsBKVX089KGVkcydsOxFSOwLddr5\nHY2JVIFF/C66CM45B264weozGUNwM9ydCqwE3gTGAMtFJHra4ctfhszt0PZvfkdiItXCha5rKbeI\n35VXWqVXYwIE85fwAtBHVU9W1ZOA84AXQxtWGcncDUufhwa94YjOfkdjIs3+/fDII9CpE6xaBXVs\nThJjChLMGEUlVV2c+0BVl4hIdJz2sfJ12LfVWhPmULNnuyJ+CxfCZZfBP/8JR0bndaTGhFowieI3\nEXkdeM97fDnRUBQwK91dYHfUmXBkd7+jMZFm61bYvh2+/BLOP9/vaIyJaMEkipuAO4DcUqs/A/8K\nWURlZdUoyNgEJ/+3+HVN+TBhgivid8cdbrB6xQpISPA7KmMiXpGJQkROAI4FPlPVp8MTUhnI3ueK\n/x15KhzV0+9ojN927IC//AXeeANatXID1ZUrW5IwJkhFVY/9K658x+XA/0SkoJnuItPqtyB9vY1N\nGNe1lJTkroe45x43NmFF/IwpkaJaFJcD7VR1j4gcCYzHnR4b2XIy3cREdbvC0Wf5HY3x07p10K+f\na0V8/jmceKLfERkTlYo6PXafqu4BUNXUYtaNHGv+DXv+cK0JEb+jMeGm6iq7Ql4Rv1mzLEkYcxiK\nOvgfIyKferfPgGMDHn9axOsOEJFeIrJMRFaKSKEzBYlIPxFREUku6Qc4SE4WLPoH1OkEDfsc1luZ\nKJSSAhde6C6eyy3id9ppVsTPmMNUVNdTv3yPXy7JG4tIPG6u7bOBFGCmiIwLvCbDW68GcCcwvSTv\nX6C1H8HuVXDqp9aaKE9ycmDUKLj3XsjKguefh1NO8TsqY2JGUXNm/3iY790FVxdqNYCIfAD0BRbn\nW+8x4Cng3sPcHmydDhWqu7pOpvzo18+NQZxxhksYxxzjd0TGxJRQjjs0AtYFPE4h3zwWItIJaKyq\nXxf1RiIyRERmicis1NTUwldM3wgJR4NEx3CKOQxZWa4lAS5RjBoFP/xgScKYEPDtiCoiccDzwLDi\n1lXVN1Q1WVWTjyyqzELGRqhydNkFaSLT/PluMqFRo9zjK66A66+37kZjQiToRCEiJT35fD1uvu1c\nid5zuWoAbYGJIvI70A0Yd1gD2hlei8LEpn374KGHoHNn+OMPq81kTJgEU2a8i4gsAFZ4j9uLSDAl\nPGYCLUWkuVdEcCAwLnehqu5Q1Xqq2kxVmwHTgAtVdVZpPgjgup6sRRGbZs50VV4ffRQGDYIlS+Di\ni/2OyphyIZgWxUvA+cBWAFWdB5xe3ItUNQu4DfgOWAJ8qKqLRORREbmw9CEXIjsDMndYiyJWpaXB\n7t0wfjy8+y7Uret3RMaUG8EUBYxT1T/k4P7f7GDeXFXH467oDnzu74Wse1ow71mojE3uX2tRxI6f\nfnJF/O680xXxW77cym8Y44NgWhTrRKQLoCISLyJ3ActDHFfJpW90/yYc5W8c5vBt3+6mIT3zTBg5\n0o1NgCUJY3wSTKK4GRgKNAE24Qadbw5lUKWSkZsorEUR1b74whXxGzPGVXy1In7G+K7YridV3Ywb\niI5suS0K63qKXmvXwqWXQuvWMG4cJB9eRRdjTNkoNlGIyChA8z+vqkNCElFp5Y5RVK7vbxymZFRh\nyhQ49VRo0sRdNNetm9VnMiaCBNP19APwo3ebCtQH9oUyqFLJ2AiV60K8HWCixtq1cN550KNHXhG/\nHj0sSRgTYYLpejpoLlER+TcwJWQRlVb6RhvIjhY5OfD66zB8uGtRvPSSFfEzJoIFc3psfs2ByDsi\n21XZ0ePii92g9dlnu+lJmzXzOyJjTBGCGaNII2+MIg7YBhQ6t4Rv0jdCve5+R2EKk5UFcXHuNmAA\n9O0LgwdbfSZjokCRiULcVXbtyavRlKOqhwxs+07VDWbbGU+Rad48uPZad23ETTe5EhzGmKhR5GC2\nlxTGq2q2d4u8JAGQtRuy91rXU6TJyIAHH3SnuaakwNH2/2NMNArmrKe5ItIx5JEcDruGIvLMmAEd\nO8Ljj8Pll7sifhdd5HdUxphSKLTrSUQqeIX9OuKmMV0F7AEE19joFKYYi5dh5Tsizs6dkJ4O334L\n557rdzTGmMNQ1BjFDKATUPaVXsuale+IDN9/D4sWwd13w1lnwbJlVn7DmBhQVKIQAFVdFaZYSi/d\nKsf6Ki0Nhg6Ft9+GNm3glltcgrAkYUxMKCpRHCkiQwtbqKrPhyCe0snYCBIPlWyOgrD79FO49VZI\nTYX774e//90ShDExpqhEEQ9Ux2tZRLSMjZBQH+Li/Y6kfFm7FgYOhLZt3YRCHSP7nAdjTOkUlSg2\nqOqjYYvkcFj5jvBRhcmToWdPV8Tvp5+ga1eoWNHvyIwxIVLU6bGR35LIZeU7wuOPP6B3bzjttLwi\nfqecYknCmBhXVKI4M2xRHC67Kju0cnLg5ZfdQPWUKfCvf7my4MaYcqHQridV3RbOQEpNc1yisBZF\n6Fx0EXz5pbseYuRIaNrU74iMMWFUmuqxkWV/GuRkWqIoa5mZEB/vivgNGgSXXAJXXmlF/Iwph4Ip\n4RHZ0u2q7DL322/QpYubMwJcorjqKksSxpRT0Z8oMqzOU5lJT3fXQnTpAhs3QuPGfkdkjIkA0d/1\nlDtXtnU9HZ5p0+Dqq2H5clcS/NlnoU4dv6MyxkSA6E8UVjm2bOzZ48Yl/vc/V6fJGGM80Z8oMjZC\nXGWoWMvvSKLPt9+6In7DhsGZZ8LSpVCpkt9RGWMiTPSPUaRvdK0JG2gN3tatrpupd2945x3Yv989\nb0nCGFOA6E8UGVa+I2iq8PHHkJQE77/vZp+bOdMShDGmSDHQ9bQJqjXzO4rosHYtXHYZtGvn5o5o\n397viIwxUSA2WhQ2kF04VVe4D9wV1RMnujOcLEkYY4IU3YkiJwsyUu3U2MKsWQPnnOMGqnOL+J10\nElSI/oakMSZ8ojtR7EsF1FoU+WVnw4svunkipk+H116zIn7GmFKL7p+WVr6jYH37wtdfQ58+rgyH\nXWFtjDkM0Z0o7KrsPIFF/K680tVnuuwyO23YGHPYQtr1JCK9RGSZiKwUkfsKWD5URBaLyHwR+VFE\nSla/2uo8ObNmQXKy62ICGDAALr/ckoQxpkyELFGISDzwCtAbSAIGiUhSvtXmAMmq2g74GHi6RBsp\n711P6ekwfLibijQ11eaJMMaERChbFF2Alaq6WlX3Ax8AfQNXUNUJqrrXezgNSCzRFjI2QoUaUKFa\nWcQbXX791Z3i+vTTrojf4sVw/vl+R2WMiUGhHKNoBKwLeJwCdC1i/euAbwpaICJDgCEATZo0yVuQ\nXo6vyk5Pd1OU/vCDO/3VGGNCJCIGs0XkCiAZ6FnQclV9A3gDIDk5WQ8sKG9zZY8f74r43XsvnHEG\nLFkCFSv6HZUxJsaFsutpPRB4Xmai99xBROQs4AHgQlXdV6ItZGwsH2c8bdkCV1wB550H//lPXhE/\nSxLGmDAIZaKYCbQUkeYiUgkYCIwLXEFEOgIjcUlic4m3kB7j5TtU4YMPoHVr+PBDeOghmDHDivgZ\nY8IqZF1PqpolIrcB3wHxwBhVXSQijwKzVHUc8AxQHfhI3Kmca1X1wqA2kJ0Bmdtju0Wxdq0rB96+\nPbz5Jpxwgt8RGWPKoZCOUajqeGB8vuf+HnC/9FOp5V5sF2stClX48Uc3y1zTpq5G04knuovpjDHG\nB9Fb6yk996rsGDrradUqdwbT2WfnFfHr1s2ShDHGV9GbKHKvyo6FrqfsbHj+ede1NHs2jBxpRfyM\nMREjIk6PLZVYKt9xwQXwzTfugrnXXoPEkl13aIwxoRS9iSK3fEfl+v7GUVr797t5IeLiYPBgV8hv\n4ECrz2SMiTjR3fVUuS7ER+GpojNmQOfO8Oqr7nH//q7aqyUJY0wEiuJEsSn6BrL37oVhw6B7d0hL\ng2OP9TsiY4wpVnR3PUXTQPaUKe6aiNWr4cYb4amnoFYtv6MyxphiRW+iyNgIdbv5HUXwcicWmjAB\nTjvN72iMMSZo0ZkoVKOjfMeXX7rCfX/5C5x+uisFXiE6d7kxpvyKzjGKrN2QvTdyu55SU900pBde\nCGPH5hXxsyRhjIlC0ZkoIrV8hyq8/74r4vfxx/DoozB9uhXxM8ZEtej8iRupU6CuXQvXXAMdO7oi\nfm3a+B2RMcYctihtUURQ+Y6cHPjuO3e/aVP4+WeYOtWShDEmZkRnokiPkPIdK1a4meZ69YLJk91z\nXbpYET9jTEyJzkSRsREkHirV9Wf7WVnwzDPQrh3Mneu6mayInzEmRkXnGEXGJkioD3E+/XI//3zX\n3dS3ryvD0bChP3EYE+EyMzNJSUkhIyPD71DKjYSEBBITE6lYhlMlR2eiSN8Y/oHsffvcHNVxcXD9\n9XDttXDppVafyZgipKSkUKNGDZo1a4bY30rIqSpbt24lJSWF5s2bl9n7Rm/XUzgHsqdNg06d4JVX\n3ONLLnGF/OyLb0yRMjIyqFu3riWJMBER6tatW+YtuOhNFOEYyN6zB+6+G046CXbtgpYtQ79NY2KM\nJYnwCsX+js6up4xNoW9R/PyzK+K3Zg3ccgs88QTUrBnabRpjTASKvhaFZkFOZugTRVaWG5OYNMl1\nOVmSMCZqff7554gIS5cuPfDcxIkTOf/88w9ab/DgwXz88ceAG4i/7777aNmyJZ06daJ79+588803\nhx3LE088QYsWLTj++OP5LvcarHx+/PFHOnXqRIcOHTjllFNYuXLlgWUffvghSUlJtGnThssuu+yw\n4wlG9LUocrLcv6EYzP78c1fE7/77XRG/RYusPpMxMWDs2LGccsopjB07lkceeSSo1/ztb39jw4YN\nLFy4kMqVK7Np0yYmTZp0WHEsXryYDz74gEWLFvHnn39y1llnsXz5cuLzXXt1880388UXX9C6dWte\nffVVRowYwdtvv82KFSt44oknmDp1KnXq1GHz5s2HFU+wou8omJPp/i3LMYpNm+D22+Gjj9yg9bBh\nrj6TJQljys7suyBtbtm+Z50O0PmfRa6ye/dupkyZwoQJE7jggguCShR79+5l1KhRrFmzhsqVKwNw\n1FFH0b9//8MK94svvmDgwIFUrlyZ5s2b06JFC2bMmEH37t0PWk9E2LlzJwA7duygoXcK/qhRo7j1\n1lupU6cOAPXrh2cq6Og7EqqXKMqi60kV3nsP7roLdu+Gxx+He+91XU7GmJjwxRdf0KtXL4477jjq\n1q3L7Nmz6dy5c5GvWblyJU2aNKFmEF3Od999NxMmTDjk+YEDB3Lfffcd9Nz69evp1i1vHp3ExETW\nr19/yGtHjx5Nnz59qFKlCjVr1mTatGkALF++HICTTz6Z7OxsHn74YXr16lVsjIcr+hJFWbYo1q51\n10QkJ7urq1u1Ovz3NMYUrJhf/qEyduxY7rzzTsAdvMeOHUvnzp0LPTuopGcNvfDCC4cdY0HvOX78\neLp27cozzzzD0KFDGT16NFlZWaxYsYKJEyeSkpJCjx49WLBgAbVr1y7zGAJFZ6KIqwwVSzmNaG4R\nv969XRG/qVNdtVerz2RMzNm2bRs//fQTCxYsQETIzs5GRHjmmWeoW7cuaWlph6xfr149WrRowdq1\na9m5c2exrYqStCgaNWrEunXrDjxOSUmhUaNGB62TmprKvHnz6Nq1KwADBgw40GpITEyka9euVKxY\nkebNm3PcccexYsUKTjzxxOB3SmmoalTdOreqq/p5Uy2VZctUTz1VFVQnTizdexhjgrZ48WJftz9y\n5EgdMmTIQc/16NFDJ02apBkZGdqsWbMDMf7+++/apEkT3b59u6qq3nvvvTp48GDdt2+fqqpu3rxZ\nP/zww8OKZ+HChdquXTvNyMjQ1atXa/PmzTUrK+ugdTIzM7Vu3bq6bNkyVVUdPXq0Xnzxxaqq+s03\n3+hVV12lqqqpqamamJioW7ZsOWQ7Be13YJaW8rgbnS2Kkp7xlJUFzz0HDz0EVarAW29Bjx6hic8Y\nEzHGjh3L8OHDD3quX79+jB07lh49evDee+9xzTXXkJGRQcWKFRk9ejS1arneihEjRvDggw+SlJRE\nQkIC1apV49FHHz2seNq0aUP//v1JSkqiQoUKvPLKKwfOeOrTpw+jR4+mYcOGjBo1in79+hEXF0ed\nOnUYM2YMAOeeey7ff/89SUlJxMfHH2gZhZq4RBM9kltW1Vmjz4aeXwT/onPPhe+/h4svdtdEHB0B\n81gYUw4sWbKE1q1b+x1GuVPQfheR2aqaXJr3i74WhWYGN5CdkeHOXoqPhyFD3K1fv9DHZ4wxMSb6\nrszOySr+1NipU6FDh7wifv36WZIwxphSir5EAYW3KHbvhjvucJMIZWSANXmN8V20dW9Hu1Ds7+hM\nFAUNZk+aBG3bwssvw223wcKFcPbZ4Y/NGHNAQkICW7dutWQRJurNR5GQkFCm7xt9YxRQeNdT1aqu\n6uvJJ4c3HmNMgRITE0lJSSE1NdXvUMqN3BnuylJ0JorcrqdPP4WlS+Gvf4WePWHBArtwzpgIknth\nmIluIe16EpFeIrJMRFaKyH0FLK8sIv/1lk8XkWZBvfF23Cxz/frBZ5/B/v3ueUsSxhhT5kKWKEQk\nHngF6A0kAYNEJCnfatcBaaraAngBeKrYN94tcEJn+OorN5nQL7+4Sq/GGGNCIpQtii7ASlVdrar7\ngQ+AvvnW6Qu8493/GDhTiqvItUXdoPW8eXDffVbp1RhjQiyUYxSNgHUBj1OAroWto6pZIrIDqAts\nCVxJRIYAQ7yH+2TKlIVW6RWAeuTbV+WY7Ys8ti/y2L7Ic3xpXxgVg9mq+gbwBoCIzCrtZeixxvZF\nHtsXeWxf5LF9kUdEZpX2taHseloPNA54nOg9V+A6IlIBqAVsDWFMxhhjSiiUiWIm0FJEmotIJWAg\nMC7fOuOAq737lwA/qV2ZY4wxESVkXU/emMNtwHdAPDBGVReJyKO4uujjgDeBf4vISmAbLpkU541Q\nxRyFbF/ksX2Rx/ZFHtsXeUq9L6KuzLgxxpjwis5aT8YYY8LGEoUxxpgiRWyiCFn5jygUxL4YKiKL\nRWS+iPwoIk39iDMcitsXAev1ExEVkZg9NTKYfSEi/b3vxiIReT/cMYZLEH8jTURkgojM8f5O+vgR\nZ6iJyBgR2SwiCwtZLiLykref5otIp6DeuLSTbYfyhhv8XgUcA1QC5gFJ+da5BXjduz8Q+K/fcfu4\nL04Hqnr3by7P+8JbrwYwGZgGJPsdt4/fi5bAHKCO97i+33H7uC/eAG727icBv/sdd4j2RQ+gE7Cw\nkOV9gG8AAboB04N530htUYSm/Ed0KnZfqOoEVd3rPZyGu2YlFgXzvQB4DFc3LCOcwYVZMPviBuAV\nVU0DUNXNYY4xXILZFwrU9O7XAv4MY3xho6qTcWeQFqYv8K4604DaItKguPeN1ERRUPmPRoWto6pZ\nQG75j1gTzL4IdB3uF0MsKnZfeE3pxqr6dTgD80Ew34vjgONEZKqITBORXmGLLryC2RcPA1eISAow\nHrg9PKFFnJIeT4AoKeFhgiMiVwDJQE+/Y/GDiMQBzwODfQ4lUlTAdT+dhmtlThaRE1R1u69R+WMQ\n8LaqPici3XHXb7VV1Ry/A4sGkdqisPIfeYLZF4jIWcADwIWqui9MsYVbcfuiBtAWmCgiv+P6YMfF\n6IB2MN+LFGCcqmaq6hpgOS5xxJpg9sV1wIcAqvorkIArGFjeBHU8yS9SE4WV/8hT7L4QkY7ASFyS\niNV+aChmX6jqDlWtp6rNVLUZbrzmQlUtdTG0CBbM38jnuNYEIlIP1xW1OpxBhkkw+2ItcCaAiLTG\nJYryOD/rOOAq7+ynbsAOVd1Q3IsisutJQ1f+I+oEuS+eAaoDH3nj+WtV9ULfgg6RIPdFuRDkvvgO\nOEdEFgPZwL2qGnOt7iD3xTBglIjcjRvYHhyLPyxFZCzux0E9bzzmIaAigKq+jhuf6QOsBPYC1wT1\nvjG4r4wxxpShSO16MsYYEyEsURhjjCmSJQpjjDFFskRhjDGmSJYojDHGFMkShYk4IpItInMDbs2K\nWLdZYZUyS7jNiV710XleyYvjS/EeN4nIVd79wSLSMGDZaBFJKuM4Z4pIhyBec5eIVD3cbZvyyxKF\niUTpqtoh4PZ7mLZ7uaq2xxWbfKakL1bV11X1Xe/hYKBhwLLrVXVxmUSZF+erBBfnXYAlClNqlihM\nVPBaDj+LyG/e7aQC1mkjIjO8Vsh8EWnpPX9FwPMjRSS+mM1NBlp4rz3Tm8NggVfrv7L3/JOSNwfI\ns95zD4vIPSJyCa7m1n+8bVbxWgLJXqvjwMHda3m8XMo4fyWgoJuIvCYis8TNPfGI99wduIQ1QUQm\neM+dIyK/evvxIxGpXsx2TDlnicJEoioB3U6fec9tBs5W1U7AAOClAl53E/CiqnbAHahTvHINA4CT\nveezgcuL2f4FwAIRSQDeBgao6gm4SgY3i0hd4P+ANqraDhgR+GJV/RiYhfvl30FV0wMWf+K9NtcA\n4INSxtkLV6Yj1wOqmgy0A3qKSDtVfQlXUvt0VT3dK+XxIHCWty9nAUOL2Y4p5yKyhIcp99K9g2Wg\nisDLXp98Nq5uUX6/Ag+ISCLwqaquEJEzgc7ATK+8SRVc0inIf0QkHfgdV4b6eGCNqi73lr8D3Aq8\njJvr4k0R+Qr4KtgPpqqpIrLaq7OzAmgFTPXetyRxVsKVbQncT/1FZAju77oBboKe+fle2817fqq3\nnUq4/WZMoSxRmGhxN7AJaI9rCR8yKZGqvi8i04HzgPEiciNuJq93VPX+ILZxeWABQRE5oqCVvNpC\nXXBF5i4BbgPOKMFn+QDoDywFPlNVFXfUDjpOYDZufOJfwMUi0hy4BzhRVdNE5G1c4bv8BPifqg4q\nQbymnLOuJxMtagEbvPkDrsQVfzuIiBwDrPa6W77AdcH8CFwiIvW9dY6Q4OcUXwY0E5EW3uMrgUle\nn34tVR2PS2DtC3jtLlzZ84J8hptpbBAuaVDSOL2Cdn8DuolIK9zsbXuAHSJyFNC7kFimASfnfiYR\nqSYiBbXOjDnAEoWJFq8CV4vIPFx3zZ4C1ukPLBSRubh5Kd71zjR6EPheROYD/8N1yxRLVTNw1TU/\nEpEFQA7wOu6g+5X3flMouI//beD13MHsfO+bBiwBmqrqDO+5EsfpjX08h6sKOw83P/ZS4H1cd1au\nN4BvRWSCqqbizsga623nV9z+NKZQVj3WGGNMkaxFYYwxpkiWKIwxxhTJEoUxxpgiWaIwxhhTJEsU\nxhhjimSJwhhjTJEsURhjjCnS/wP+F6+WLFe1SwAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"5ddvNGLCg_CQ","colab_type":"code","outputId":"860654e0-b40e-4505-f8ca-bd61dc39e5b9","executionInfo":{"status":"ok","timestamp":1581245315633,"user_tz":-330,"elapsed":1127,"user":{"displayName":"Shivesh Jha","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCOcob5sxhbLtDPP9IdJeirG9gSY-Oe_l6T9cnTKw=s64","userId":"01027328504624407555"}},"colab":{"base_uri":"https://localhost:8080/","height":255}},"source":["from sklearn.model_selection import StratifiedKFold, KFold\n","scoring = ['precision_macro', 'recall_macro', 'roc_auc']\n","#scoring = 'roc_auc'\n","skf = StratifiedKFold(n_splits=10)\n","scores = ms.cross_validate(KNN, X_train, y_train, scoring=scoring,\n","                        cv=skf, return_train_score=False)\n","\n","def print_format(f,x,y,z):\n","    print('Fold %2d    %4.3f        %4.3f      %4.3f' % (f, x, y, z))\n","\n","def print_cv(scores):\n","    fold = [x + 1 for x in range(len(scores['test_precision_macro']))]\n","    print('         Precision     Recall       AUC')\n","    [print_format(f,x,y,z) for f,x,y,z in zip(fold, scores['test_precision_macro'], \n","                                          scores['test_recall_macro'],\n","                                          scores['test_roc_auc'])]\n","    print('-' * 40)\n","    print('Mean       %4.3f        %4.3f      %4.3f' % \n","          (np.mean(scores['test_precision_macro']), np.mean(scores['test_recall_macro']), np.mean(scores['test_roc_auc'])))  \n","    print('Std        %4.3f        %4.3f      %4.3f' % \n","          (np.std(scores['test_precision_macro']), np.std(scores['test_recall_macro']), np.std(scores['test_roc_auc'])))\n","\n","print_cv(scores)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["         Precision     Recall       AUC\n","Fold  1    0.795        0.767      0.815\n","Fold  2    0.859        0.841      0.894\n","Fold  3    0.770        0.748      0.824\n","Fold  4    0.829        0.838      0.885\n","Fold  5    0.764        0.754      0.833\n","Fold  6    0.799        0.788      0.879\n","Fold  7    0.760        0.718      0.836\n","Fold  8    0.866        0.828      0.848\n","Fold  9    0.801        0.752      0.882\n","Fold 10    0.828        0.794      0.859\n","----------------------------------------\n","Mean       0.807        0.783      0.855\n","Std        0.036        0.040      0.027\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DnOwmyBag-_x","colab_type":"code","outputId":"579c2aa3-b18d-4a97-f4a8-b4c0748bfcef","executionInfo":{"status":"ok","timestamp":1581245649571,"user_tz":-330,"elapsed":1980,"user":{"displayName":"Shivesh Jha","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCOcob5sxhbLtDPP9IdJeirG9gSY-Oe_l6T9cnTKw=s64","userId":"01027328504624407555"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["nr.seed(123)\n","inside = ms.StratifiedKFold(n_splits=10, shuffle = True)\n","nr.seed(321)\n","outside = ms.StratifiedKFold(n_splits=10, shuffle = True)\n","\n","nr.seed(3456)\n","## Define the dictionary for the grid search and the model object to search on\n","param_grid = {\"n_neighbors\": [1,2,3,4,5,6,7,8,9,10]}\n","\n","## Perform the grid search over the parameters\n","clf = ms.GridSearchCV(estimator = KNN, param_grid = param_grid, \n","                      cv = inside, # Use the inside folds\n","                      scoring = 'roc_auc',\n","                      return_train_score = True)\n","\n","## Fit the cross validated grid search over the data \n","clf.fit(X_train, y_train)\n","\n","## And print the best parameter value\n","print(clf.best_params_)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{'n_neighbors': 9}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ix3ERXcNg-8l","colab_type":"code","outputId":"1efcc47d-f417-4e0e-a795-81ec9db1cf27","executionInfo":{"status":"ok","timestamp":1581245754200,"user_tz":-330,"elapsed":1057,"user":{"displayName":"Shivesh Jha","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCOcob5sxhbLtDPP9IdJeirG9gSY-Oe_l6T9cnTKw=s64","userId":"01027328504624407555"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["KNN = KNeighborsClassifier(n_neighbors=9)\n","KNN = KNN.fit(X_train, y_train)\n","KNN"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n","                     metric_params=None, n_jobs=None, n_neighbors=9, p=2,\n","                     weights='uniform')"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"eaTSv3H2g-5I","colab_type":"code","outputId":"d4333bc6-a28d-4f22-d2bf-f83272e67bf5","executionInfo":{"status":"ok","timestamp":1581245757888,"user_tz":-330,"elapsed":1046,"user":{"displayName":"Shivesh Jha","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCOcob5sxhbLtDPP9IdJeirG9gSY-Oe_l6T9cnTKw=s64","userId":"01027328504624407555"}},"colab":{"base_uri":"https://localhost:8080/","height":255}},"source":["from sklearn.model_selection import StratifiedKFold, KFold\n","scoring = ['precision_macro', 'recall_macro', 'roc_auc']\n","#scoring = 'roc_auc'\n","scores = ms.cross_validate(KNN, X_train, y_train, scoring=scoring,\n","                        cv=outside, return_train_score=False)\n","print_cv(scores)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["         Precision     Recall       AUC\n","Fold  1    0.784        0.788      0.850\n","Fold  2    0.779        0.721      0.817\n","Fold  3    0.898        0.792      0.887\n","Fold  4    0.915        0.883      0.924\n","Fold  5    0.855        0.836      0.893\n","Fold  6    0.819        0.802      0.890\n","Fold  7    0.816        0.745      0.890\n","Fold  8    0.893        0.841      0.914\n","Fold  9    0.752        0.726      0.737\n","Fold 10    0.814        0.773      0.752\n","----------------------------------------\n","Mean       0.833        0.790      0.855\n","Std        0.053        0.050      0.063\n"],"name":"stdout"}]}]}