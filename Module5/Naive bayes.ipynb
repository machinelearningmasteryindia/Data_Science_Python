{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Naive bayes.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"8m8MwTE9E9cO","colab_type":"text"},"source":["# **Naive Bayes**"]},{"cell_type":"markdown","metadata":{"id":"bTG9KYKdDkPi","colab_type":"text"},"source":["## **Introduction**\n","\n","Naive Bayes is a probabilistic machine learning algorithm that can be used in a wide variety of classification tasks. Typical applications include filtering spam, classifying documents, sentiment prediction etc. It is based on the works of Rev. Thomas Bayes (1702-61) and hence the name.\n","\n","But why is it called ‘Naive’?\n","\n","> The name **naive** is used because it assumes the features that go into the model is independent of each other. \n","\n","That is changing the value of one feature, does not directly influence or change the value of any of the other features used in the algorithm.\n","\n","Since it is a probabilistic model, the algorithm can be coded up easily and the predictions made real quick. Because of this, it is easily scalable and is trditionally the algorithm of choice for real-world applications (apps) that are required to respond to user’s requests instantaneously.\n","\n","But before we go into Naive Bayes, we need to understand what 'Conditional Probability' is and what is the 'Bayes Rule'."]},{"cell_type":"markdown","metadata":{"id":"b8AolDrSEH-d","colab_type":"text"},"source":["## **Conditional Probability**\n","Lets start from the basics by understanding conditional probability.\n","\n","**Playing Cards Example**\n","\n","If we pick a card from the deck, can we guess the probability of getting a queen given the card is a spade?\n","\n","Well, we have already set a condition that the card is a spade. So, the denominator is 13 and not 52. And since there is only one queen in spades, the probability it is a queen given the card is a spade is 1/13 = 0.077\n","\n","This is a classic example of conditional probability. So, when we say the conditional probability of A given B, it denotes the probability of A occurring given that B has already occurred.\n","\n","Mathematically, Conditional probability of A given B can be computed as: P(A|B) = P(A AND B) / P(B)\n","\n","**School Example**\n","\n","Let’s see a slightly complicated example. Consider a school with a total population of 100 persons. These 100 persons can be seen either as ‘Students’ and ‘Teachers’ or as a population of ‘Males’ and ‘Females’.\n","\n","With below tabulation of the 100 people, what is the conditional probability that a certain member of the school is a ‘Teacher’ given that he is a ‘Man’?\n","\n","![School Example](https://drive.google.com/drive/folders/1zyJmBUGtJLswPCnfQkSAw86j8-pZPO6F/school_new.png)\n","\n","\n","$$P(Teacher | Male) = \\frac{P(Teacher \\cap  Male)}{P(Male) = 12/60 = 0.2}$$\n","\n","$$P(A | B) = \\frac{P(A \\cap  B)}{P(B)}$$"]},{"cell_type":"markdown","metadata":{"id":"L_nCuQbzLD1h","colab_type":"text"},"source":["## **Bayes rule**\n","\n","The Bayes Rule is a way of going from P(X|Y), known from the training dataset, to find P(Y|X).\n","\n","To do this, we replace A and B in the above formula, with the feature X and response Y.\n","\n","For observations in test or scoring data, the X would be known while Y is unknown. And for each row of the test dataset, you want to compute the probability of Y given the X has already happened.\n","\n","What happens if Y has more than 2 categories? we compute the probability of each class of Y and let the highest win. Hence, bayes rule can be defined as:\n","\n","$$P(Y | X) = \\frac{P(X \\cap  Y) * P(Y)}{P(X)}$$"]},{"cell_type":"markdown","metadata":{"id":"Wgi-u97eL3vr","colab_type":"text"},"source":["## **The Naive Bayes**\n","\n","The Bayes Rule provides the formula for the probability of Y given X. But, in real-world problems, you typically have multiple X variables.\n","\n","When the features are independent, we can extend the Bayes Rule to what is called Naive Bayes.\n","\n","It is called ‘Naive’ because of the naive assumption that the X’s are independent of each other. Regardless of its name, it’s a powerful formula.\n","\n","$$P(Y = k| X) = \\frac{P(X \\cap  Y=k) * P(Y=k)}{P(X)}$$ becomes, Naive Bayes\n","\n","$$P(Y = k| X_1....X_n) = \\frac{P(X_1 \\cap  Y=k)P(X_2 \\cap  Y=k)....P(X_n \\cap  Y=k) * P(Y=k)}{P(X_1)P(X_2)....P(X_n)}$$\n","\n","In technical jargon, the left-hand-side (LHS) of the equation is understood as the posterior probability or simply the posterior\n","\n","The RHS has 2 terms in the numerator.\n","\n","The first term is called the ‘Likelihood of Evidence’. It is nothing but the conditional probability of each X’s given Y is of particular class ‘c’.\n","\n","Since all the X’s are assumed to be independent of each other, you can just multiply the ‘likelihoods’ of all the X’s and called it the ‘Probability of likelihood of evidence’. This is known from the training dataset by filtering records where Y=c.\n","\n","The second term is called the prior which is the overall probability of Y=c, where c is a class of Y. In simpler terms, \n","$$Prior = count(Y=c) / n_{Records}$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xhw5GV8KNJTd","colab_type":"text"},"source":["## **Naive Bayes Example by Hand**\n","\n","Say we have 1000 fruits which could be either ‘banana’, ‘orange’ or ‘other’. These are the 3 possible classes of the Y variable.\n","\n","We have data for the following X variables, all of which are binary (1 or 0).\n","\n"," + Long\n"," + Sweet\n"," + Yellow\n","\n","The first few rows of the training dataset look like this:\n","\n","|Fruit|long|Sweet|Yellow|\n","|-----|-----|-----|-----|\n","|banana|1|0|1|\n","|orange|0|1|0|\n","|other|1|1|0|\n","\n","For the sake of computing the probabilities, let’s aggregate the training data to form a counts table like this\n","\n","So the objective of the classifier is to predict if a given fruit is a ‘Banana’ or ‘Orange’ or ‘Other’ when only the 3 features (long, sweet and yellow) are known.\n","\n","Let’s say you are given a fruit that is: Long, Sweet and Yellow, can you predict what fruit it is?\n","\n","This is the same of predicting the Y when only the X variables in testing data are known. Let’s solve it by hand using Naive Bayes.\n","\n","The idea is to compute the 3 probabilities, that is the probability of the fruit being a banana, orange or other. Whichever fruit type gets the highest probability wins.\n","\n","All the information to calculate these probabilities is present in the above tabulation.\n","\n","**Step 1:** Compute the ‘Prior’ probabilities for each of the class of fruits.\n","\n","That is, the proportion of each fruit class out of all the fruits from the population. You can provide the ‘Priors’ from prior information about the population. Otherwise, it can be computed from the training data.\n","\n","For this case, let’s compute from the training data. Out of 1000 records in training data, you have 500 Bananas, 300 Oranges and 200 Others. So the respective priors are 0.5, 0.3 and 0.2.\n","\n","P(Y=Banana) = 500 / 1000 = 0.50\n","\n","P(Y=Orange) = 300 / 1000 = 0.30\n","\n","P(Y=Other) = 200 / 1000 = 0.20\n","\n","**Step 2:** Compute the probability of evidence that goes in the denominator.\n","\n","This is nothing but the product of P of Xs for all X. This is an optional step because the denominator is the same for all the classes and so will not affect the probabilities.\n","\n","P(x1=Long) = 500 / 1000 = 0.50\n","\n","P(x2=Sweet) = 650 / 1000 = 0.65\n","\n","P(x3=Yellow) = 800 / 1000 = 0.80\n","\n","**Step 3:** Compute the probability of likelihood of evidences that goes in the numerator.\n","\n","It is the product of conditional probabilities of the 3 features. If you refer back to the formula, it says P(X1 |Y=k). Here X1 is ‘Long’ and k is ‘Banana’. That means the probability the fruit is ‘Long’ given that it is a Banana. In the above table, you have 500 Bananas. Out of that 400 is long. So, P(Long | Banana) = 400/500 = 0.8.\n","\n","Here, I have done it for Banana alone.\n","\n","Probability of Likelihood for Banana\n","\n","P(x1=Long | Y=Banana) = 400 / 500 = 0.80\n","\n","P(x2=Sweet | Y=Banana) = 350 / 500 = 0.70\n","\n","P(x3=Yellow | Y=Banana) = 450 / 500 = 0.90\n","\n","So, the overall probability of Likelihood of evidence for Banana = 0.8 * 0.7 * 0.9 = 0.504\n","\n","Step 4: Substitute all the 3 equations into the Naive Bayes formula, to get the probability that it is a banana.\n","\n","$$P(Banana|Long, Sweet and yellow) = P(Long|Banana)*P(Sweet|Banana)*P(Yellow|Banana)*P(Banana) = 0.504 * 0.5 = 0.252$$\n","\n","$$P(Banana|Long, Sweet and yellow) = P(Long|Banana)*P(Sweet|Banana)*P(Yellow|Banana)*P(Banana) = 0.504 * 0.5 = 0.252$$\n","\n","$$P(Orange|Long, Sweet and yellow) = P(Long|Banana)*P(Sweet|Banana)*P(Yellow|Banana)*P(Banana) = 0$$\n","\n","$$P(Other|Long, Sweet and yellow) = P(Long|Banana)*P(Sweet|Banana)*P(Yellow|Banana)*P(Banana) = 0.01875$$\n","\n","Hence, Answer is Banana as it has highest probabilites."]},{"cell_type":"markdown","metadata":{"id":"bNWA_etsZO8Q","colab_type":"text"},"source":["## **Laplace Correction**\n","\n","The value of P(Orange | Long, Sweet and Yellow) was zero in the above example, because, P(Long | Orange) was zero. That is, there were no ‘Long’ oranges in the training data.\n","\n","It makes sense, but when you have a model with many features, the entire probability will become zero because one of the feature’s value was zero. To avoid this, we increase the count of the variable with zero to a small value (usually 1) in the numerator, so that the overall probability doesn’t become zero.\n","\n","This correction is called ‘Laplace Correction’. Most Naive Bayes model implementations accept this or an equivalent form of correction as a parameter."]},{"cell_type":"markdown","metadata":{"id":"IDwXnmfnZzYN","colab_type":"text"},"source":["## **Gaussian Naive Bayes**\n","\n","So far we’ve seen the computations when the X’s are categorical. But how to compute the probabilities when X is a continuous variable?\n","\n","If we assume that the X follows a particular distribution, then you can plug in the probability density function of that distribution to compute the probability of likelihoods.\n","\n","If you assume the X’s follow a Normal (aka Gaussian) Distribution, which is fairly common, we substitute the corresponding probability density of a Normal distribution and call it the Gaussian Naive Bayes. You need just the mean and variance of the X to compute this formula.\n","\n","$$P(X|Y=c) = \\frac{1}{\\sqrt{2\\pi\\sigma_c^2}} e^{\\frac{-(x-\\mu_c)^2}{2\\sigma_c^2}}$$\n","\n","where mu and sigma are the mean and variance of the continuous X computed for a given class ‘c’ (of Y).\n","\n","To make the features more Gaussian like, you might consider transforming the variable using something like the Box-Cox to achieve this."]},{"cell_type":"code","metadata":{"id":"5Z-W8Zvx7bKV","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UHpfHBsOExuR","colab_type":"text"},"source":["## References\n","\n"," 1. [Naive Bayes](https://www.machinelearningplus.com/predictive-modeling/how-naive-bayes-algorithm-works-with-example-and-full-code/)"]}]}