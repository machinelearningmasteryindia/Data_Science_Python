{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ensemble Modeling.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"JsqaV81Tgrq9","colab_type":"text"},"source":["# **Ensemble Modeling**\n","\n","Ensemble modeling is a process where multiple diverse base models are used to predict an outcome. The motivation for using ensemble models is to reduce the generalization error of the prediction. As long as the base models are diverse and independent, the prediction error decreases when the ensemble approach is used. The approach seeks the wisdom of crowds in making a prediction. Even though the ensemble model has multiple base models within the model, it acts and performs as a single model. Most of the practical data science applications utilize ensemble modeling techniques.\n","\n","> Ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone…. Wikipedia\n","\n","The fundamental principle of the ensemble model is that a group of weak learners come together to form a strong learner, which increases the accuracy of the model. When we try to predict the target variable by any machine learning technique, the main causes of the difference between the actual and predicted values are noise, variance and bias. The set reduces these factors (except noise, which is an irreducible error).\n"]},{"cell_type":"markdown","metadata":{"id":"w6cBmYC7mGqj","colab_type":"text"},"source":["## **Simple Ensemble Techniques**\n","\n","In this section, we will look at a few simple but powerful techniques, namely:\n","\n"," + Max Voting\n"," + Averaging\n"," + Weighted Averaging"]},{"cell_type":"markdown","metadata":{"id":"PA2rd0YPmQ2D","colab_type":"text"},"source":["### **Max Voting**\n","\n","The max voting method is generally used for classification problems. In this technique, multiple models are used to make predictions for each data point. The predictions by each model are considered as a ‘vote’. The predictions which we get from the majority of the models are used as the final prediction.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9f7-lSMzml5k","colab_type":"text"},"source":["### **Averaging**\n","\n","Similar to the max voting technique, multiple predictions are made for each data point in averaging. In this method, we take an average of predictions from all the models and use it to make the final prediction. Averaging can be used for making predictions in regression problems or while calculating probabilities for classification problems."]},{"cell_type":"markdown","metadata":{"id":"bYgEg7mQmqrS","colab_type":"text"},"source":["### **Weighted Averaging**\n","\n","This is an extension of the averaging method. All models are assigned different weights defining the importance of each model for prediction. For instance, if two of your colleagues are critics, while others have no prior experience in this field, then the answers by these two friends are given more importance as compared to the other people."]},{"cell_type":"markdown","metadata":{"id":"TJJm-MU5m5lV","colab_type":"text"},"source":["## **Advanced Ensemble techniques**\n","\n","Below are some advanced ensemble techniques:\n","\n"," + Bagging\n"," + Boosting\n"," + Stacking"]},{"cell_type":"markdown","metadata":{"id":"ppippjk5McuM","colab_type":"text"},"source":["### **Bagging**\n","\n","Bootstrap Aggregating is an ensemble method. First, we create random samples of the training data set with replacment (sub sets of training data set). Then, we build a model (classifier or Decision tree) for each sample. Finally, results of these multiple models are combined using average or majority voting.\n","\n","> As each model is exposed to a different subset of data and we use their collective output at the end, so we are making sure that problem of overfitting is taken care of by not clinging too closely to our training data set. Thus, Bagging helps us to reduce the variance error.\n","\n","\n","*Combinations of multiple models decreases variance, especially in the case of unstable models, and may produce a more reliable prediction than a single model.*\n","\n","Bagging is a simple and a very powerful ensemble method. It is a general procedure that can be used to reduce our model’s variance. A higher variance means that your model is overfitted. Certain algorithms such as decision trees usually suffer from high variance. In another way, decision trees are extremely sensitive to the data on which they have been trained. If the underlying data is changed even a little bit, then the resulting decision tree can be very different and as result our model’s predictions will change drastically. Bagging offers a solution to the problem of high variance. It can systematically reduce overfitting by taking an average of several decision trees. Bagging uses bootstrap sampling and finally aggregates the individual models by averaging to get the ultimate predictions. **Bootstrap sampling simply means sampling rows at random from the training dataset with replacement.**\n","\n","![](https://miro.medium.com/max/850/0*lfH2wc6V2osrRBXk.png)\n","\n","With bagging, it is therefore possible that you draw a single training example more than once. This results in a modified version of the training set where some rows are represented multiple times and some are absent. This also lets you create new data, which is similar to the data you started with. By doing this, you can fit many different but similar models.\n","Specifically, the way bagging works is as follows:\n","\n","Step 1: You draw B samples with replacement from the original data set where B is a number less than or equal to n, the total number of samples in the training set.\n","\n","![](https://miro.medium.com/max/1012/1*7NAo4D12sROHXDt1IovWfw.png)\n","\n","\n","Step 2: Train a decision trees on newly created bootstrapped samples. Repeat the Step1 and Step2 any number of times that you like. Generally, higher the number of trees, the better the model. But remember! Excess number of trees can make a model complicated and ultimately lead to overfitting as your model starts seeing relationships in the data that do not exist in the first place.\n","\n","![](https://miro.medium.com/max/1086/1*l16JAxJR5MJea12jut-FLQ.png)\n","\n","To generate a prediction using the bagged trees approach, you have to generate a prediction from each of the decision trees, and then simply average the predictions together to get a final prediction. Bagged or ensemble prediction is the average prediction across the sampled bootstrapped trees. Your bagged trees model works very similar to the council. Usually, when a council needs to take a decision, it simply considers a majority vote. The option that gets more votes (say- option A got 100 votes and option B got 90 votes), is the ultimately the council’s final decision. Similarly, in bagging, when you are trying to solve a problem of classification, you are basically taking a majority vote of all your decision trees. And, in case of regression we simply take an average of all our decision tree predictions. The collective knowledge of a diverse set of decision trees typically beats the knowledge of any individual tree. Bagged trees therefore offer better predictive performance.\n","\n","**Algorithms :**\n","\n"," 1. ExtraTree\n"," 2. Randomforest\n"," 3. Bagging Clasifier/Bagging Regressor\n","\n","**Random Forests**\n","\n","Random forest is different from the vanilla bagging in just one way. It uses a modified tree learning algorithm that inspects, at each split in the learning process, a random subset of the features. We do so to avoid the correlation between the trees. Suppose that we have a very strong predictor in the data set along with a number of other moderately strong predictors, then in the collection of bagged trees, most or all of our decision trees will use the very strong predictor for the first split! All bagged trees will look similar. Hence all the predictions from the bagged trees will be highly correlated. Correlated\n","predictors cannot help in improving the accuracy of prediction. By taking a random subset of features, Random Forests systematically avoids correlation and improves model’s performance. The example below illustrates how Random Forest algorithm works.\n","\n","![](https://miro.medium.com/max/884/1*5vlUF8FRR6flPPWK4wt-Kw.png)\n","\n","Let’s look at a case when we are trying to solve a classification problem. As evident from the image above, our training data has four features- Feature1, Feature 2, Feature 3 and Feature 4. Now, each of our bootstrapped sample will be trained on a particular subset of features. For example, Decision Tree 1 will be trained on features 1 and 4 . DT2 will be trained on features 2 and 4, and finally DT3 will be trained on features 3 and 4. We will therefore have 3 different models, each trained on a different subset of features. We will finally feed in our new test data into each of these models, and get a unique prediction. The prediction that gets the maximum number of votes will be the ultimate decision of the random forest algorithm. For example, DT1 and DT3 predicted a positive class for a particular instance of our test data, while DT2 predicted a negative class. Since, the positive class got the majority number of votes(2), our random forest will ultimately classify this instance as positive. Again, I would like to stress on how the Random Forest algorithm uses a random subset of features to train several models, each model seeing only specific subset of the dataset.\n","\n","Random forest is one of the most widely used ensemble learning algorithms. Why is it so effective? The reason is that by using multiple samples of the original dataset, we reduce the variance of the final model. Remember that the low variance means low overfitting. Overfitting happens when our model tries to explain small variations in the dataset because our dataset is just a small sample of the population of all possible examples of the phenomenon we try to model. If we were unlucky with how our training set was sampled, then it could contain some undesirable (but unavoidable) artifacts: noise, outliers and over- or underrepresented examples. By creating multiple random samples with replacement of our training set, we reduce the effect of these artifacts.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"K7nvY4nURYug","colab_type":"text"},"source":["### **Boosting**\n","\n","Boosting methods work in the same spirit as bagging methods: we build a family of models that are aggregated to obtain a strong learner that performs better. However, unlike bagging that mainly aims at reducing variance, boosting is a technique that consists in fitting sequentially multiple weak learners in a very adaptative way: each model in the sequence is fitted giving more importance to observations in the dataset that were badly handled by the previous models in the sequence. Intuitively, each new model focus its efforts on the most difficult observations to fit up to now, so that we obtain, at the end of the process, a strong learner with lower bias (even if we can notice that boosting can also have the effect of reducing variance). Boosting, like bagging, can be used for regression as well as for classification problems.\n","\n","Being mainly focused at reducing bias, the base models that are often considered for boosting are models with low variance but high bias. For example, if we want to use trees as our base models, we will choose most of the time shallow decision trees with only a few depths. Another important reason that motivates the use of low variance but high bias models as weak learners for boosting is that these models are in general less computationally expensive to fit (few degrees of freedom when parametrised). Indeed, as computations to fit the different models can’t be done in parallel (unlike bagging), it could become too expensive to fit sequentially several complex models.\n","\n","Once the weak learners have been chosen, we still need to define how they will be sequentially fitted (what information from previous models do we take into account when fitting current model?) and how they will be aggregated (how do we aggregate the current model to the previous ones?). We will discuss these questions in the two following subsections, describing more especially two important boosting algorithms: adaboost and gradient boosting.\n","\n","In a nutshell, these two meta-algorithms differ on how they create and aggregate the weak learners during the sequential process. Adaptive boosting updates the weights attached to each of the training dataset observations whereas gradient boosting updates the value of these observations. This main difference comes from the way both methods try to solve the optimisation problem of finding the best model that can be written as a weighted sum of weak learners.\n","\n","![](https://miro.medium.com/max/1400/0*yDz8euzLbQvucBwx.png)\n","\n","**Similarities and differences between Bagging and Boosting**\n","\n","**Similarities**\n","\n","+ Uses voting\n","+ Combines model of same type\n","\n","**Differences**\n","\n","+ Individual models built seprately in bagging whereas in boosting each new model get infulenced previous weak learner.\n","+ Equal weight is given to each model in bagging whereas in boosting weights a models contribution by its contribution.\n","\n","**Algorithams :**\n","\n","1. Adaboost(Adaboost)\n","2. Gradient boosting(GBM)\n","3. Extreme Gradient boosting(Xgboost)\n","4. Catboost\n","5. light gradient boostng(light GBM)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"p6y5RuItnHdV","colab_type":"text"},"source":["### **Stacking**\n","\n","Stacking is an ensemble learning technique that uses predictions from multiple models (for example decision tree, knn or svm) to build a new model. This model is used for making predictions on the test set. Below is a step-wise explanation for a simple stacked ensemble:\n","\n"," 1. A model (for ex: Logistic regression) is fitted and tuned on the given dataset using all standard ML approaches(cross validation, feature selection, hyper-parameter tuning). After sucessful tuning, prediction will be generated on training data using this final model. This prediction will be used as feature for future steps.\n"," 2. Above process is repeated for another model(for ex: Decision tree). We can select some of best performing models.\n"," 3. Another ML model will be then used based on Predicted values of above models/"]},{"cell_type":"markdown","metadata":{"id":"Fv44PsYIUveL","colab_type":"text"},"source":["## **Advantages/Benefits of ensemble methods**\n","\n","Ensemble methods are used in almost all the ML hackathons to enhance the prediction abilities of the models. Let’s take a look at the advantages of using ensemble methods:\n","\n","1. **More accurate prediction results :** We can compare the working of the ensemble methods to the Diversification of our financial portfolios. It is advised to keep a mixed portfolio across debt and equity to reduce the variability and hence, to minimize the risk. Similarly, the ensemble of models will give better performance on the test case scenarios (unseen data) as compared to the individual models in most of the cases.\n","\n","2. **Stable and more robust model:** The aggregate result of multiple models is always less noisy than the individual models. This leads to model stability and robustness.\n","\n","3. Ensemble models can be used to capture the linear as well as the non-linear relationships in the data.This can be accomplished by using 2 different models and forming an ensemble of the two.\n","\n","**Disadvantages of ensemble methods**\n","\n","1. **Reduction in model interpret-ability:** Using ensemble methods reduces the model interpret-ability due to increased complexity and makes it very difficult to draw any crucial business insights at the end.\n","2. **Computation and design time is high** It is not good for real time applications.\n","3. The selection of models for creating an ensemble is an art which is really hard to master.\n"]},{"cell_type":"markdown","metadata":{"id":"vTVZMeUQvQJX","colab_type":"text"},"source":["## **Example: Titanic Survival prediction**"]},{"cell_type":"markdown","metadata":{"id":"B5qpJGJezzts","colab_type":"text"},"source":["### **Max Voting**"]},{"cell_type":"code","metadata":{"id":"JxEyRhBjgeDO","colab_type":"code","colab":{}},"source":["# Setting the path\n","import os\n","os.chdir(\"/content/drive/My Drive/Introduction to Data Science - Python edition/dataset/titanic\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HUfu88bSvfqZ","colab_type":"code","colab":{}},"source":["# Loading libraries\n","import numpy as np\n","import pandas as pd\n","import sklearn.model_selection as ms\n","import sklearn.metrics as sklm\n","import numpy.random as nr\n","import matplotlib.pyplot as plt\n","import math\n","\n","%matplotlib inline\n","\n","pd.set_option('display.max_rows', 100)\n","pd.set_option('display.max_columns', 100)\n","pd.set_option('display.float_format', lambda x: '%.2f' % x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Z5z4YWJvfnz","colab_type":"code","colab":{}},"source":["# load npy file\n","X_train = np.load('X_train.npy')\n","X_validation = np.load('X_validation.npy')\n","X_test = np.load('X_test.npy')\n","\n","y_train = np.load('y_train.npy')\n","y_validation = np.load('y_validation.npy')\n","y_test = np.load('y_test.npy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qf7zkwjavflq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1583664744319,"user_tz":-330,"elapsed":1048,"user":{"displayName":"Shivesh Jha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_Uc5KcZfzFa1TTXvWfAHzxoVGsQqVXlAeWLfsqA=s64","userId":"01027328504624407555"}},"outputId":"91016023-d7f8-4ade-f02a-06b50f090ecd"},"source":["# Max voting\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn import metrics as sklm\n","\n","model1= LogisticRegression()\n","model2 = KNeighborsClassifier()\n","model3 = DecisionTreeClassifier()\n","\n","model1.fit(X_train,y_train)\n","model2.fit(X_train,y_train)\n","model3.fit(X_train,y_train)\n","\n","pred1=model1.predict(X_validation)\n","pred2=model2.predict(X_validation)\n","pred3=model3.predict(X_validation)\n","\n","f1_score_1 = sklm.f1_score(y_validation, pred1)\n","f1_score_2 = sklm.f1_score(y_validation, pred2)\n","f1_score_3 = sklm.f1_score(y_validation, pred3)\n","\n","print(f1_score_1)\n","print(f1_score_2)\n","print(f1_score_3)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.7961165048543688\n","0.7474747474747475\n","0.7184466019417477\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JYTy3xOrxpoC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1583665166432,"user_tz":-330,"elapsed":1219,"user":{"displayName":"Shivesh Jha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_Uc5KcZfzFa1TTXvWfAHzxoVGsQqVXlAeWLfsqA=s64","userId":"01027328504624407555"}},"outputId":"5e0c888a-4f18-4b05-80c4-02b21c381648"},"source":["import statistics\n","final_pred = np.array([])\n","for i in range(0,len(X_validation)):\n","    final_pred = np.append(final_pred, statistics.mode([pred1[i], pred2[i], pred3[i]]))\n","\n","f1_score_4 = sklm.f1_score(y_validation, final_pred)\n","f1_score_4"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8118811881188118"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"asv9B6iRzQaE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1583665269977,"user_tz":-330,"elapsed":1031,"user":{"displayName":"Shivesh Jha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_Uc5KcZfzFa1TTXvWfAHzxoVGsQqVXlAeWLfsqA=s64","userId":"01027328504624407555"}},"outputId":"010017a4-d37c-49f8-ba93-3d3a1c3e12aa"},"source":["from sklearn.ensemble import VotingClassifier\n","\n","model = VotingClassifier(estimators=[('lr', model1), ('KNN', model2), ('dt', model3)], voting='hard')\n","model.fit(X_train,y_train)\n","final_pred = model.predict(X_validation)\n","f1_score_4 = sklm.f1_score(y_validation, final_pred)\n","f1_score_4"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8118811881188118"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"7VYSY1Ogz7G9","colab_type":"text"},"source":["### **Bagging**"]},{"cell_type":"markdown","metadata":{"id":"Fs8qR-391x3G","colab_type":"text"},"source":["#### **Bagging classifier**"]},{"cell_type":"code","metadata":{"id":"a-OiW0Y9115k","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1583666117812,"user_tz":-330,"elapsed":1017,"user":{"displayName":"Shivesh Jha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_Uc5KcZfzFa1TTXvWfAHzxoVGsQqVXlAeWLfsqA=s64","userId":"01027328504624407555"}},"outputId":"4dac14d0-4ac1-4761-fac1-a013d0cd745a"},"source":["from sklearn.ensemble import BaggingClassifier\n","\n","BC = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100)\n","BC.fit(X_train, y_train)\n","y_pred_BC = BC.predict(X_validation)\n","\n","f1_score_BC = sklm.f1_score(y_validation, y_pred_BC)\n","f1_score_BC"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7722772277227722"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"markdown","metadata":{"id":"FJSn1xX026Lq","colab_type":"text"},"source":["#### **Extratree classifier**"]},{"cell_type":"code","metadata":{"id":"Th_OOvDT2_m8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1583666265235,"user_tz":-330,"elapsed":1136,"user":{"displayName":"Shivesh Jha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_Uc5KcZfzFa1TTXvWfAHzxoVGsQqVXlAeWLfsqA=s64","userId":"01027328504624407555"}},"outputId":"a58331df-9946-436f-d9bb-a3c251270110"},"source":["from sklearn.ensemble import ExtraTreesClassifier\n","\n","ET = ExtraTreesClassifier(n_estimators = 100)\n","ET.fit(X_train, y_train)\n","y_pred_ET = ET.predict(X_validation)\n","\n","f1_score_ET = sklm.f1_score(y_validation, y_pred_ET)\n","f1_score_ET"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7524752475247524"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"markdown","metadata":{"id":"2I5pDGYu1pjH","colab_type":"text"},"source":["#### **Random forest**"]},{"cell_type":"code","metadata":{"id":"jqcDmYbPz-Cs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1583665721359,"user_tz":-330,"elapsed":1901,"user":{"displayName":"Shivesh Jha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_Uc5KcZfzFa1TTXvWfAHzxoVGsQqVXlAeWLfsqA=s64","userId":"01027328504624407555"}},"outputId":"ac2f806c-750e-404b-8ae4-86b87a317528"},"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","RF = RandomForestClassifier(random_state = 123, n_estimators = 500)\n","RF.fit(X_train, y_train)\n","y_pred_RF = RF.predict(X_validation)\n","\n","f1_score_RF = sklm.f1_score(y_validation, y_pred_RF)\n","f1_score_RF"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7843137254901961"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"if_18PYq3nie","colab_type":"text"},"source":["### **Boosting**"]},{"cell_type":"markdown","metadata":{"id":"__XQOr4M3q2P","colab_type":"text"},"source":["#### **Adaboost**"]},{"cell_type":"code","metadata":{"id":"ZXzNI_qm3uHH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1583666407619,"user_tz":-330,"elapsed":1929,"user":{"displayName":"Shivesh Jha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_Uc5KcZfzFa1TTXvWfAHzxoVGsQqVXlAeWLfsqA=s64","userId":"01027328504624407555"}},"outputId":"6af14e5b-f13d-4148-db5f-e5f2f65ec1f4"},"source":["from sklearn.ensemble import AdaBoostClassifier\n","\n","AB = AdaBoostClassifier(random_state = 123, n_estimators = 500)\n","AB.fit(X_train, y_train)\n","y_pred_AB = AB.predict(X_validation)\n","\n","f1_score_AB = sklm.f1_score(y_validation, y_pred_AB)\n","f1_score_AB"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8269230769230769"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"markdown","metadata":{"id":"gE_bd_r93_-K","colab_type":"text"},"source":["#### **Gradient boosting**"]},{"cell_type":"code","metadata":{"id":"waL0D8ou4C1_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1583666518179,"user_tz":-330,"elapsed":1148,"user":{"displayName":"Shivesh Jha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_Uc5KcZfzFa1TTXvWfAHzxoVGsQqVXlAeWLfsqA=s64","userId":"01027328504624407555"}},"outputId":"f5e3f80b-bf3a-4ea4-e842-76b28c109d0c"},"source":["from sklearn.ensemble import GradientBoostingClassifier\n","\n","GB = GradientBoostingClassifier(random_state = 123, n_estimators = 100)\n","GB.fit(X_train, y_train)\n","y_pred_GB = GB.predict(X_validation)\n","\n","f1_score_GB = sklm.f1_score(y_validation, y_pred_GB)\n","f1_score_GB"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.836734693877551"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"markdown","metadata":{"id":"ffksB3oE4cic","colab_type":"text"},"source":["#### **Extreme Gradient boosting**"]},{"cell_type":"code","metadata":{"id":"uzEMq9V94hSX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"status":"ok","timestamp":1583667463077,"user_tz":-330,"elapsed":1514,"user":{"displayName":"Shivesh Jha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_Uc5KcZfzFa1TTXvWfAHzxoVGsQqVXlAeWLfsqA=s64","userId":"01027328504624407555"}},"outputId":"3cb5f80c-37bb-4d84-ad89-8d66f31d9d27"},"source":["from xgboost import XGBClassifier\n","\n","XGB = XGBClassifier(random_state = 123, n_estimators = 500)\n","print(XGB)\n","XGB.fit(X_train, y_train)\n","y_pred_XGB = XGB.predict(X_validation)\n","\n","f1_score_XGB = sklm.f1_score(y_validation, y_pred_XGB)\n","print(sklm.confusion_matrix(y_validation, y_pred_XGB))\n","print(sklm.accuracy_score(y_validation, y_pred_XGB))\n","print(sklm.classification_report(y_validation, y_pred_XGB))\n","print(f1_score_XGB)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n","              colsample_bynode=1, colsample_bytree=1, gamma=0,\n","              learning_rate=0.1, max_delta_step=0, max_depth=3,\n","              min_child_weight=1, missing=None, n_estimators=500, n_jobs=1,\n","              nthread=None, objective='binary:logistic', random_state=123,\n","              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n","              silent=None, subsample=1, verbosity=1)\n","[[74  8]\n"," [ 9 43]]\n","0.8731343283582089\n","              precision    recall  f1-score   support\n","\n","           0       0.89      0.90      0.90        82\n","           1       0.84      0.83      0.83        52\n","\n","    accuracy                           0.87       134\n","   macro avg       0.87      0.86      0.87       134\n","weighted avg       0.87      0.87      0.87       134\n","\n","0.8349514563106797\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qeFj2Vzs5YRz","colab_type":"text"},"source":["### **Stacking**"]},{"cell_type":"code","metadata":{"id":"47cpO8Pw5baY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1583667181808,"user_tz":-330,"elapsed":7275,"user":{"displayName":"Shivesh Jha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_Uc5KcZfzFa1TTXvWfAHzxoVGsQqVXlAeWLfsqA=s64","userId":"01027328504624407555"}},"outputId":"f9500ab4-1075-4245-ecd8-e307e052b126"},"source":["from sklearn.ensemble import StackingClassifier\n","\n","estimators = [\n","              ('XGB', XGBClassifier(random_state = 123, n_estimators = 500)),\n","              ('GB', GradientBoostingClassifier(random_state = 123, n_estimators = 100)),\n","              ('AB', AdaBoostClassifier(random_state = 123, n_estimators = 500))]\n","\n","SC = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n","SC.fit(X_train, y_train)\n","y_pred_SC = SC.predict(X_validation)\n","\n","f1_score_SC = sklm.f1_score(y_validation, y_pred_SC)\n","f1_score_SC"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8282828282828283"]},"metadata":{"tags":[]},"execution_count":55}]}]}